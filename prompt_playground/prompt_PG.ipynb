{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452713c3",
   "metadata": {},
   "source": [
    "# Imports and Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db8067f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asim_aims_ac_za/Home/Aide-playground/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root added to sys.path: /home/asim_aims_ac_za/Home/Aide-playground\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Dict, Any, List\n",
    "import shutil # For file operations\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from rich.console import Console\n",
    "from rich.syntax import Syntax\n",
    "from funcy import select_values # Used in generation function\n",
    "\n",
    "notebook_dir = Path().resolve()\n",
    "project_root = notebook_dir.parent # Should be 'aide-ds' root\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Project Root added to sys.path: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da71c44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- AIDE Imports ---\n",
    "try:\n",
    "    from aide.interpreter import Interpreter, ExecutionResult\n",
    "    from aide.utils.response import extract_code, format_code\n",
    "    from aide.utils import serialize\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing AIDE modules: {e}\")\n",
    "    print(\"Ensure AIDE is installed (e.g., `pip install -e .` from aide-ds root)\")\n",
    "    raise # Stop execution if imports fail\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - [%(levelname)s] - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)] # Log to notebook output\n",
    ")\n",
    "logger = logging.getLogger(\"PromptPlaygroundNotebook\")\n",
    "# Optional: Add file handler if you still want a separate log file\n",
    "log_file_path = notebook_dir / \"playground_notebook.log\"\n",
    "file_handler = logging.FileHandler(log_file_path)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - [%(levelname)s] - %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# --- Rich Console ---\n",
    "console = Console()\n",
    "\n",
    "print(\"Setup Complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99ffca",
   "metadata": {},
   "source": [
    "#  Helper Functions (Model Loading, Prompt Parsing, Generation)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "547599c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_CACHE: Dict[str, Tuple[Any, Any]] = {}\n",
    "\n",
    "def load_model_and_tokenizer(model_id: str, load_in_4bit: bool = True) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Loads model and tokenizer, caching them.\"\"\"\n",
    "    global MODEL_CACHE # Declare modification of global cache\n",
    "    if model_id in MODEL_CACHE:\n",
    "        logger.info(f\"Using cached model/tokenizer for {model_id}\")\n",
    "        return MODEL_CACHE[model_id]\n",
    "\n",
    "    logger.info(f\"Loading model and tokenizer: {model_id} (4-bit: {load_in_4bit})\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            logger.info(f\"Set tokenizer pad_token to eos_token: {tokenizer.eos_token}\")\n",
    "        tokenizer.padding_side = \"left\"\n",
    "\n",
    "        quantization_config = None\n",
    "        compute_dtype = torch.float16 # Default compute type\n",
    "        if load_in_4bit and torch.cuda.is_available():\n",
    "            if torch.cuda.is_bf16_supported():\n",
    "                compute_dtype = torch.bfloat16\n",
    "                logger.info(\"BF16 supported, using for compute.\")\n",
    "            else:\n",
    "                logger.info(\"BF16 not supported, using float16 for compute.\")\n",
    "\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=compute_dtype,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "            logger.info(\"Using 4-bit quantization (BitsAndBytesConfig).\")\n",
    "        elif load_in_4bit:\n",
    "            logger.warning(\"CUDA not available, cannot load in 4-bit. Loading in default precision.\")\n",
    "        else:\n",
    "             logger.info(\"4-bit quantization disabled. Loading in default precision.\")\n",
    "\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=compute_dtype if quantization_config else None, # Match compute type if quantized\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        logger.info(f\"Model '{model_id}' loaded successfully to device: {model.device}\")\n",
    "        MODEL_CACHE[model_id] = (model, tokenizer) # Cache it\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed to load model {model_id}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "790285a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_prompt_file(filepath: Path) -> Tuple[Optional[str], str]:\n",
    "    \"\"\"Parses prompt file into system and user parts.\"\"\"\n",
    "    try:\n",
    "        content = filepath.read_text()\n",
    "        separator = \"---USER---\"\n",
    "        if separator in content:\n",
    "            system_prompt, user_prompt = content.split(separator, 1)\n",
    "            return system_prompt.strip(), user_prompt.strip()\n",
    "        else:\n",
    "            logger.warning(f\"Separator '{separator}' not found in {filepath}. Using entire content as user prompt.\")\n",
    "            return None, content.strip()\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Prompt file not found: {filepath}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading prompt file {filepath}: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e75af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_code_batch(\n",
    "    model: Any,\n",
    "    tokenizer: Any,\n",
    "    system_prompt: Optional[str],\n",
    "    user_prompt: str,\n",
    "    num_responses: int,\n",
    "    generation_params: Dict[str, Any]\n",
    ") -> List[str]:\n",
    "    \"\"\"Generates a batch of code responses using the loaded model.\"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    try:\n",
    "        prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        logger.info(\"Applied chat template to prompt for batch generation.\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not apply chat template for batch generation (error: {e}). Using basic concatenation.\")\n",
    "        prompt_text = (system_prompt + \"\\n\\n\" if system_prompt else \"\") + user_prompt\n",
    "\n",
    "    logger.info(f\"Input prompt (start) for batch generation:\\n{prompt_text[:300]}...\")\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    prompt_token_length = input_ids.shape[1]\n",
    "\n",
    "    do_sample = generation_params.get(\"temperature\", 0.2) > 0.0 or generation_params.get(\"top_p\", None) is not None or generation_params.get(\"top_k\", None) is not None\n",
    "    if num_responses > 1 and not do_sample:\n",
    "         logger.warning(\"Forcing temperature to 0.2 for multiple distinct responses.\")\n",
    "         generation_params[\"temperature\"] = 0.2\n",
    "         do_sample = True\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"temperature\": generation_params.get(\"temperature\", 0.2),\n",
    "        \"max_new_tokens\": generation_params.get(\"max_new_tokens\", 2048),\n",
    "        \"top_p\": generation_params.get(\"top_p\"),\n",
    "        \"top_k\": generation_params.get(\"top_k\"),\n",
    "        \"repetition_penalty\": generation_params.get(\"repetition_penalty\"),\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"num_return_sequences\": num_responses,\n",
    "    }\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    logger.info(f\"Generating {num_responses} responses with config: {gen_kwargs}\")\n",
    "    t0 = time.time()\n",
    "    outputs = []\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            generated_outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **gen_kwargs\n",
    "            )\n",
    "        for i in range(num_responses):\n",
    "            output_ids = generated_outputs[i][prompt_token_length:]\n",
    "            output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "            outputs.append(output_text.strip())\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error during batch generation\")\n",
    "        return [f\"Error during generation: {e}\"] * num_responses\n",
    "    t1 = time.time()\n",
    "    logger.info(f\"Batch generation of {num_responses} responses took {t1-t0:.2f} seconds.\")\n",
    "    return outputs\n",
    "\n",
    "print(\"Helper functions defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97218d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.5: Unload Model Function\n",
    "import gc # Make sure gc is imported\n",
    "\n",
    "def unload_model(model_id_to_unload: Optional[str]):\n",
    "    \"\"\"Attempts to unload a model and tokenizer from memory and VRAM.\"\"\"\n",
    "    global MODEL_CACHE, model, tokenizer # Need global to modify/delete notebook-level vars\n",
    "\n",
    "    if model_id_to_unload and model_id_to_unload in MODEL_CACHE:\n",
    "        logger.info(f\"Attempting to unload model: {model_id_to_unload}\")\n",
    "        try:\n",
    "            # Remove from cache first\n",
    "            m, t = MODEL_CACHE.pop(model_id_to_unload)\n",
    "\n",
    "            # Clear global references if they point to the unloaded model\n",
    "            if 'model' in globals() and model is m:\n",
    "                del model\n",
    "            if 'tokenizer' in globals() and tokenizer is t:\n",
    "                del tokenizer\n",
    "\n",
    "            # Delete the specific references we retrieved\n",
    "            del m\n",
    "            del t\n",
    "\n",
    "            # Aggressive cleanup\n",
    "            gc.collect() # Run Python garbage collection\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                logger.info(\"Clearing CUDA cache...\")\n",
    "                torch.cuda.empty_cache() # Ask PyTorch to release unused cached memory\n",
    "                logger.info(\"CUDA cache cleared.\")\n",
    "            else:\n",
    "                 logger.info(\"CUDA not available, skipping cache clear.\")\n",
    "\n",
    "            logger.info(f\"Successfully initiated unloading for {model_id_to_unload}.\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during unloading of {model_id_to_unload}: {e}\", exc_info=True)\n",
    "            # Put it back in cache if deletion failed? Maybe not, keep it simple.\n",
    "            return False\n",
    "    else:\n",
    "        logger.info(f\"Model {model_id_to_unload} not found in cache or not specified, nothing to unload.\")\n",
    "        return True # Nothing to do is still success in a way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d331de",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ee672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-13 17:43:27,800 - [WARNING] - Separator '---USER---' not found in /home/asim_aims_ac_za/Home/Aide-playground/prompt_playground/real_prompt.txt. Using entire content as user prompt.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Model ---\n",
    "model_id = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\n",
    "# model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "# model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\" # smaller model for faster testing if needed\n",
    "load_in_4bit = True # Set based on your VRAM\n",
    "\n",
    "# --- Prompt ---\n",
    "# Option 1: Load from file\n",
    "prompt_file = notebook_dir / \"real_prompt.txt\"\n",
    "system_prompt, user_prompt = parse_prompt_file(prompt_file)\n",
    "# Option 2: Define directly\n",
    "# system_prompt = \"You are a Python coding assistant.\"\n",
    "# user_prompt = \"Write Python code to read './input/train.csv' and print the shape.\"\n",
    "\n",
    "# --- Data Path  ---\n",
    "REAL_DATA_DIR = project_root / \"prompt_playground/example_tasks/house_prices\" # Example using AIDE's sample task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac161cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct (4-bit: True)\n",
      "Prompt File: /home/asim_aims_ac_za/Home/Aide-playground/prompt_playground/real_prompt.txt\n",
      "Real Data Dir: /home/asim_aims_ac_za/Home/Aide-playground/prompt_playground/example_tasks/house_prices\n",
      "Output Dir: /home/asim_aims_ac_za/Home/Aide-playground/prompt_playground/playground_notebook_results\n",
      "Num Responses: 2\n",
      "Generation Params: {'temperature': 0.6, 'max_new_tokens': 2048, 'top_p': 0.95, 'top_k': None, 'repetition_penalty': 1.1}\n",
      "Execution Timeout: 120s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Generation ---\n",
    "num_responses_to_generate = 2 # How many code variations to generate\n",
    "generation_params = {\n",
    "    \"temperature\": 0.6,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": None,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "}\n",
    "\n",
    "# --- Execution ---\n",
    "execution_timeout = 120 # Seconds\n",
    "\n",
    "# --- Output ---\n",
    "output_base_dir = notebook_dir / \"playground_notebook_results\" # Directory for results\n",
    "\n",
    "# --- Display Config ---\n",
    "print(f\"Using Model: {model_id} (4-bit: {load_in_4bit})\")\n",
    "print(f\"Prompt File: {prompt_file}\")\n",
    "print(f\"Real Data Dir: {REAL_DATA_DIR}\")\n",
    "print(f\"Output Dir: {output_base_dir}\")\n",
    "print(f\"Num Responses: {num_responses_to_generate}\")\n",
    "print(f\"Generation Params: {generation_params}\")\n",
    "print(f\"Execution Timeout: {execution_timeout}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b5a4a",
   "metadata": {},
   "source": [
    "# Load Model (Re-run this cell to change models)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c455b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Loading new model: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">'deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct'</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mLoading new model: \u001b[0m\u001b[1;34m'deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct'\u001b[0m\u001b[1;34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-13 17:44:54,833 - [INFO] - Loading model and tokenizer: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct (4-bit: True)\n",
      "2025-04-13 17:44:55,678 - [INFO] - BF16 supported, using for compute.\n",
      "2025-04-13 17:44:55,681 - [INFO] - Using 4-bit quantization (BitsAndBytesConfig).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct:\n",
      "- configuration_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct:\n",
      "- modeling_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Fetching 4 files:   0%|          | 0/4 [01:12<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_model_id = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\n",
    "load_in_4bit = True \n",
    "\n",
    "\n",
    "# --- Logic to handle model switching ---\n",
    "# Define these globally if they don't exist yet\n",
    "if 'current_model_id' not in globals():\n",
    "    current_model_id = None\n",
    "if 'model' not in globals():\n",
    "    model = None\n",
    "if 'tokenizer' not in globals():\n",
    "    tokenizer = None\n",
    "\n",
    "# Check if we need to switch models\n",
    "if current_model_id != target_model_id:\n",
    "    if current_model_id is not None:\n",
    "        console.print(f\"[bold yellow]Switching model: Unloading '{current_model_id}'...[/bold yellow]\")\n",
    "        if unload_model(current_model_id):\n",
    "            # Reset global variables after successful unload attempt\n",
    "            current_model_id = None\n",
    "            model = None\n",
    "            tokenizer = None\n",
    "            console.print(f\"[green]Unload initiated for '{current_model_id}'.[/green]\")\n",
    "        else:\n",
    "             console.print(f\"[bold red]Error unloading '{current_model_id}'. Check logs. Cannot load new model.[/bold red]\")\n",
    "             # Optional: raise an error here or exit cell execution\n",
    "             raise RuntimeError(f\"Failed to unload model {current_model_id}\")\n",
    "\n",
    "    # Load the new model if different from current or if nothing is loaded\n",
    "    console.print(f\"[bold blue]Loading new model: '{target_model_id}'...[/bold blue]\")\n",
    "    model, tokenizer = load_model_and_tokenizer(target_model_id, load_in_4bit=load_in_4bit)\n",
    "    current_model_id = target_model_id # Update the tracker\n",
    "    console.print(f\"[bold green]Model '{target_model_id}' and tokenizer loaded.[/bold green]\")\n",
    "\n",
    "else:\n",
    "    console.print(f\"[bold yellow]Model '{target_model_id}' is already loaded. Skipping reload.[/bold yellow]\")\n",
    "\n",
    "# --- Sanity check ---\n",
    "if model is None or tokenizer is None:\n",
    "     console.print(\"[bold red]Error: Model or Tokenizer failed to load correctly.[/bold red]\")\n",
    "else:\n",
    "     console.print(f\"Currently loaded model: [bold cyan]{current_model_id}[/bold cyan] on device: [cyan]{model.device}[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73040071",
   "metadata": {},
   "source": [
    "# Generate Responses (Re-run this cell to test generation)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce16ed00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'console' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconsole\u001b[49m\u001b[38;5;241m.\u001b[39mrule(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[bold blue]Generating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_responses_to_generate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Responses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Make sure model/tokenizer are loaded\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'console' is not defined"
     ]
    }
   ],
   "source": [
    "console.rule(f\"[bold blue]Generating {num_responses_to_generate} Responses\")\n",
    "\n",
    "# Make sure model/tokenizer are loaded\n",
    "if 'model' not in locals() or 'tokenizer' not in locals():\n",
    "     raise NameError(\"Model and tokenizer not loaded. Run Cell 4 first.\")\n",
    "if 'system_prompt' not in locals() or 'user_prompt' not in locals():\n",
    "     raise NameError(\"Prompts not defined. Run Cell 3 first.\")\n",
    "\n",
    "all_raw_responses = generate_code_batch(\n",
    "    model, tokenizer, system_prompt, user_prompt,\n",
    "    num_responses_to_generate, generation_params\n",
    ")\n",
    "\n",
    "console.print(f\"[bold green]Generated {len(all_raw_responses)} responses.[/bold green]\")\n",
    "# Optionally print previews\n",
    "for i, r in enumerate(all_raw_responses):\n",
    "   print(f\"\\n--- Response {i+1} Preview ---\")\n",
    "   print(r[:300] + \"...\"+\"\\n\")\n",
    "   print(\"___________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da7434",
   "metadata": {},
   "source": [
    "# Process & Execute Responses (Re-run this cell after generating)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f682d6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No responses generated yet. Run Cell 5 first.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'all_raw_responses' not in locals() or not all_raw_responses:\n",
    "    print(\"No responses generated yet. Run Cell 5 first.\")\n",
    "else:\n",
    "    output_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results_summary = []\n",
    "\n",
    "    # Ensure REAL_DATA_DIR exists before starting the loop\n",
    "    if not REAL_DATA_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Real data directory not found: {REAL_DATA_DIR}\")\n",
    "\n",
    "    for i, raw_response in enumerate(all_raw_responses):\n",
    "        console.rule(f\"[bold blue]Processing Response {i+1}/{len(all_raw_responses)}\")\n",
    "        response_dir = output_base_dir / f\"response_{i}\"\n",
    "        response_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # --- Setup Isolated Workspace ---\n",
    "        workspace_dir = response_dir / \"workspace\"\n",
    "        if workspace_dir.exists():\n",
    "            shutil.rmtree(workspace_dir)\n",
    "        workspace_dir.mkdir(exist_ok=True)\n",
    "        input_dir = workspace_dir / \"input\"\n",
    "        input_dir.mkdir(exist_ok=True) # Create input dir first\n",
    "        submission_dir = workspace_dir / \"submission\"; submission_dir.mkdir(exist_ok=True)\n",
    "        working_dir = workspace_dir / \"working\"; working_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # --- Copy REAL Data ---\n",
    "        logger.info(f\"Copying data from {REAL_DATA_DIR} to {input_dir}\")\n",
    "        try:\n",
    "            # Use copytree for robustness with directories\n",
    "            shutil.copytree(REAL_DATA_DIR, input_dir, dirs_exist_ok=True)\n",
    "            # Optional: Preprocess if needed (like unzipping in AIDE)\n",
    "            # from aide.utils import preproc_data\n",
    "            # preproc_data(input_dir)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to copy data for response {i+1}: {e}\", exc_info=True)\n",
    "            console.print(f\"[bold red]Response {i+1}: FAILED to copy data. Skipping execution.[/bold red]\")\n",
    "            continue\n",
    "\n",
    "        # --- Save Raw Response ---\n",
    "        raw_response_path = response_dir / \"raw_response.txt\"\n",
    "        raw_response_path.write_text(raw_response, errors='ignore') # Ignore encoding errors just in case\n",
    "        logger.info(f\"Raw response {i+1} saved to: {raw_response_path}\")\n",
    "        console.print(f\"[bold cyan]Raw Response {i+1}:[/bold cyan]\\n{raw_response[:500]}...\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        # --- Extract Code ---\n",
    "        extracted_code = extract_code(raw_response)\n",
    "        exec_result = None\n",
    "        summary = {\"index\": i, \"success\": False, \"error\": None, \"output_preview\": \"\", \"code_path\": None, \"log_path\": None}\n",
    "\n",
    "        if not extracted_code:\n",
    "            logger.error(f\"Response {i+1}: Code extraction FAILED.\")\n",
    "            console.print(\"[bold red]Code extraction FAILED.[/bold red]\")\n",
    "            summary[\"error\"] = \"Code Extraction Failed\"\n",
    "        else:\n",
    "            formatted_extracted_code = format_code(extracted_code)\n",
    "            code_path = response_dir / \"extracted_code.py\"\n",
    "            code_path.write_text(formatted_extracted_code)\n",
    "            summary[\"code_path\"] = str(code_path)\n",
    "            logger.info(f\"Extracted code {i+1} saved to: {code_path}\")\n",
    "            console.print(f\"[bold green]Extracted Code {i+1}:[/bold green]\")\n",
    "            console.print(Syntax(formatted_extracted_code, \"python\", theme=\"default\", line_numbers=True, word_wrap=True))\n",
    "            print(\"-\" * 10)\n",
    "\n",
    "            # --- Execute Code ---\n",
    "            logger.info(f\"Executing code for response {i+1}...\")\n",
    "            # Use the workspace directory specific to this response run\n",
    "            interpreter = Interpreter(working_dir=workspace_dir, timeout=execution_timeout)\n",
    "            try:\n",
    "                exec_result = interpreter.run(formatted_extracted_code, reset_session=True)\n",
    "                summary[\"success\"] = exec_result.exc_type is None\n",
    "                summary[\"error\"] = exec_result.exc_type\n",
    "                summary[\"output_preview\"] = \"\\n\".join(exec_result.term_out[:10]) + (\"...\" if len(exec_result.term_out) > 10 else \"\")\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Interpreter failed for response {i+1}: {e}\", exc_info=True)\n",
    "                 summary[\"error\"] = f\"Interpreter Error: {e}\"\n",
    "                 exec_result = ExecutionResult(term_out=[f\"Interpreter Error: {e}\"], exec_time=0, exc_type=type(e).__name__)\n",
    "            finally:\n",
    "                try:\n",
    "                    interpreter.cleanup_session()\n",
    "                except Exception as e_clean:\n",
    "                     logger.error(f\"Error cleaning up interpreter for response {i+1}: {e_clean}\")\n",
    "\n",
    "            # --- Save Execution Log ---\n",
    "            exec_log_path = response_dir / \"execution_log.json\"\n",
    "            summary[\"log_path\"] = str(exec_log_path)\n",
    "            try:\n",
    "                 # Ensure exec_result exists before saving\n",
    "                 if exec_result:\n",
    "                     serialize.dump_json(exec_result, exec_log_path)\n",
    "                     logger.info(f\"Execution log saved to: {exec_log_path}\")\n",
    "                 else:\n",
    "                     logger.warning(f\"No execution result to save for response {i+1}\")\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Failed to save execution log: {e}\")\n",
    "                 if exec_result:\n",
    "                      exec_log_path.with_suffix(\".txt\").write_text(str(exec_result))\n",
    "\n",
    "        results_summary.append(summary)\n",
    "        console.print(f\"[bold magenta]Execution Result {i+1}:[/bold magenta]\")\n",
    "        if exec_result:\n",
    "             console.print(f\"  Success: {exec_result.exc_type is None}\")\n",
    "             console.print(f\"  Execution Time: {exec_result.exec_time:.2f}s\")\n",
    "             if exec_result.exc_type:\n",
    "                 console.print(f\"  Error Type: [bold red]{exec_result.exc_type}[/bold red]\")\n",
    "             term_out_str = \"\\n\".join(exec_result.term_out) if isinstance(exec_result.term_out, list) else str(exec_result.term_out)\n",
    "             console.print(f\"  Terminal Output (preview):\")\n",
    "             console.print(\"[dim]\" + term_out_str[:500] + (\"\\n...\" if len(term_out_str) > 500 else \"\") + \"[/dim]\")\n",
    "        else:\n",
    "             console.print(\"  Execution skipped or failed before result generation.\")\n",
    "        console.print(f\"  Full output/logs saved in: {response_dir}\")\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c35275",
   "metadata": {},
   "source": [
    "# Final Summary \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Final Summary ---\n",
    "console.rule(\"[bold green]Run Summary\")\n",
    "for res in results_summary:\n",
    "    status = \"[green]Success[/green]\" if res[\"success\"] else f\"[red]Failed ({res['error']})[/red]\"\n",
    "    console.print(f\"Response {res['index']}: Status={status}\")\n",
    "    if res[\"code_path\"]: console.print(f\"  Code: {res['code_path']}\")\n",
    "    if res[\"log_path\"]: console.print(f\"  Log:  {res['log_path']}\")\n",
    "    # Only show preview if failed and there is output\n",
    "    if not res[\"success\"] and res[\"output_preview\"]: console.print(f\"  Output Preview: [dim]{res['output_preview']}[/dim]\")\n",
    "\n",
    "console.print(\"\\nPlayground iteration finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aide-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
