# aide/utils/wandb_logger.py
import logging
import shutil
from pathlib import Path
import pandas as pd
import time 
import re
import json
from typing import Optional, Dict, Any 
try:
    import wandb
    from omegaconf import OmegaConf 
except ImportError:
    wandb = None
    OmegaConf = None 

from aide.utils.config import Config 
from aide.journal import Journal, Node 
from . import copytree # Assuming this is your utility for copying directory contents
from ..utils.metric import WorstMetricValue # For type checking
from ..utils.metrics_calculator import save_logs_to_wandb
logger = logging.getLogger("aide.wandb") 

class WandbLogger:
    def __init__(self, cfg: Config, app_logger: logging.Logger, competition_benchmarks: Optional[Dict[str, Any]] = None):
        self.cfg = cfg
        self.app_logger = app_logger # Main application logger for internal messages
        self.wandb_run = None
        self.app_logger = app_logger 
        self.competition_benchmarks = competition_benchmarks
        
        # Attributes to store data for plots across steps
        self._metric_hist: list[float] = []           # History of valid metrics from non-buggy steps
        self._bug_flags: list[int] = []               # 1 if buggy, 0 if not, for each step
        self._sub_produced_flags: list[int] = []      # 1 if submission.csv produced by non-buggy code this step, else 0
        self._above_median_flags: list[int] = []      # 1 if non-buggy metric > median_threshold
        self._gold_medal_flags: list[int] = []
        self._silver_medal_flags: list[int] = []
        self._bronze_medal_flags: list[int] = []
        self._effective_fix_flags: list[int] = []     # 1 if a fix (debug/reflection) was effective this step
        self._effective_reflection_flags: list[int] = [] # 1 if reflection specifically led to a fix

        # Directory to store a copy of each submission.csv generated by a non-buggy step
        self.submission_history_dir = Path(self.cfg.log_dir) / "submission_history_per_step"
        # accumulator for every step’s log dict (so we can dump it locally later)
        self._step_logs: list[Dict[str, Any]] = []

    def _sanitize_artifact_name_component(self, name_component: str) -> str:
        sanitized = re.sub(r'[^a-zA-Z0-9_.-]+', '_', name_component)
        sanitized = re.sub(r'^[^a-zA-Z0-9]+', '', sanitized)
        sanitized = re.sub(r'[^a-zA-Z0-9_]+$', '', sanitized)
        if not sanitized: 
            return "default_component"
        return sanitized

    def init_wandb(self):
        if wandb and OmegaConf and self.cfg.wandb.enabled:
            try:
                resolved_cfg_container = OmegaConf.to_container(self.cfg, resolve=True, throw_on_missing=False)
                self.submission_history_dir.mkdir(parents=True, exist_ok=True) 
                
                self.wandb_run = wandb.init(
                    project=self.cfg.wandb.project,
                    entity=self.cfg.wandb.entity,
                    name=self.cfg.wandb.run_name,
                    config=resolved_cfg_container, 
                    job_type="aide_run",
                    tags=["aide-agent", self.cfg.agent.ITS_Strategy, self.cfg.agent.code.model, self.cfg.competition_name],
                )
                self.app_logger.info(f"W&B run initialized: {self.wandb_run.url if self.wandb_run else 'Failed'}")
            except Exception as e:
                self.app_logger.error(f"Failed to initialize W&B: {e}", exc_info=True)
                self.wandb_run = None
        elif not OmegaConf:
            self.app_logger.error("OmegaConf is not available. Cannot serialize config for W&B.")
            self.wandb_run = None

    def log_step_data(self, 
                      base_step_log_data: dict, # Basic scalar data from Agent
                      result_node: Node,        # The finalized node for this step
                      current_step_number: int,
                      current_submission_dir: Path): # Path to ./submission for current step
        if not self.wandb_run or not wandb:
            return

        current_log_data = base_step_log_data.copy()
        log_prefix_step = f"WANDB_LOG_STEP_{current_step_number}"

        try:
            # --- Update internal accumulators based on the FINALIZED result_node ---
            is_buggy_final = result_node.is_buggy
            metric_val_final = result_node.metric.value if not is_buggy_final and result_node.metric and result_node.metric.value is not None else float('nan')
            
            submission_csv_current_step = current_submission_dir / "submission.csv"
            submission_produced_this_step = 1 if submission_csv_current_step.exists() and not is_buggy_final else 0

            # Archive submission if produced and non-buggy
            if submission_produced_this_step == 1:
                try:
                    archive_sub_path = self.submission_history_dir / f"step_{current_step_number}_submission.csv"
                    shutil.copy2(submission_csv_current_step, archive_sub_path)
                    logger.debug(f"{log_prefix_step}: Archived submission to {archive_sub_path}", extra={"verbose": True})
                except Exception as e_copy:
                    logger.error(f"{log_prefix_step}: Error archiving submission for step {current_step_number}: {e_copy}")
            
            # Update scalars in current_log_data to reflect final state
            current_log_data["eval/is_buggy"] = 1 if is_buggy_final else 0
            current_log_data["eval/submission_produced_this_step"] = submission_produced_this_step # Log for this specific step
            current_log_data["eval/validation_metric"] = metric_val_final

            # Append to internal lists for cumulative plots
            self._bug_flags.append(1 if is_buggy_final else 0)
            self._sub_produced_flags.append(submission_produced_this_step) # Tracks if sub was made by non-buggy code
            self._effective_fix_flags.append(1 if result_node.effective_debug_step else 0)
            self._effective_reflection_flags.append(1 if result_node.effective_reflections else 0)
            
            # Medal flags - only if not buggy and metric is valid
            if not is_buggy_final and not pd.isna(metric_val_final):
                # self._metric_hist.append(metric_val_final) # Already done above this block
                if self.competition_benchmarks:
                    logger.debug(f"{log_prefix_step}: Calculating medals. Metric: {metric_val_final:.4f}", extra={"verbose": True})
                    
                    gold_achieved = 1 if metric_val_final >= self.competition_benchmarks.get("gold_threshold", float('inf')) else 0
                    silver_achieved = 0 # Default to 0
                    bronze_achieved = 0 # Default to 0

                    if not gold_achieved: # Only check for silver if not gold
                        silver_achieved = 1 if metric_val_final >= self.competition_benchmarks.get("silver_threshold", float('inf')) else 0
                        if not silver_achieved: # Only check for bronze if not gold or silver
                            bronze_achieved = 1 if metric_val_final >= self.competition_benchmarks.get("bronze_threshold", float('inf')) else 0
                    
                    # Above median can be independent
                    above_median_achieved = 1 if metric_val_final >= self.competition_benchmarks.get("median_threshold", float('inf')) else 0
                    
                    self._above_median_flags.append(above_median_achieved)
                    self._gold_medal_flags.append(gold_achieved)
                    self._silver_medal_flags.append(silver_achieved)
                    self._bronze_medal_flags.append(bronze_achieved)

                    logger.debug(f"{log_prefix_step}: Medals for this step - Median: {above_median_achieved}, Gold: {gold_achieved}, Silver: {silver_achieved}, Bronze: {bronze_achieved}", extra={"verbose": True})
                else: 
                    logger.warning(f"{log_prefix_step}: No competition_benchmarks available, cannot calculate medal flags for this step.", extra={"verbose": True})
                    self._above_median_flags.append(0); self._gold_medal_flags.append(0)
                    self._silver_medal_flags.append(0); self._bronze_medal_flags.append(0)
            else: 
                self._above_median_flags.append(0); self._gold_medal_flags.append(0)
                self._silver_medal_flags.append(0); self._bronze_medal_flags.append(0)

            # --- Generate Cumulative Plots ---
            # Buggy vs Clean
            if self._bug_flags: # Check if list is not empty
                bug_count = sum(self._bug_flags); clean_count = len(self._bug_flags) - bug_count
                bug_table = wandb.Table(data=[["Buggy", bug_count], ["Clean", clean_count]], columns=["label", "count"])
                current_log_data["plots/bug_vs_clean_cumulative"] = wandb.plot.bar(bug_table, "label", "count", title="Buggy vs Clean Steps (Cumulative)")

            # Submission Presence (Cumulative for non-buggy steps)
            if self._sub_produced_flags:
                produced_count = sum(self._sub_produced_flags)
                not_produced_count = len(self._sub_produced_flags) - produced_count # Count of non-buggy steps that didn't produce
                sub_table = wandb.Table(data=[["Submission Produced (Non-Buggy)", produced_count], ["No Submission (Non-Buggy)", not_produced_count]], columns=["label", "count"])
                current_log_data["plots/submission_presence_cumulative"] = wandb.plot.bar(sub_table, "label", "count", title="Submission Presence by Non-Buggy Steps (Cumulative)")

            # Medals (Cumulative)
            if self.competition_benchmarks:
                if self._above_median_flags:
                    true_count = sum(self._above_median_flags); false_count = len(self._above_median_flags) - true_count
                    table = wandb.Table(data=[["Above Median", true_count], ["Not Above Median", false_count]], columns=["label", "count"])
                    current_log_data["plots/above_median_bar_cumulative"] = wandb.plot.bar(table, "label", "count", title="Above Median Steps (Cumulative)")
                if self._gold_medal_flags:
                    true_count = sum(self._gold_medal_flags); false_count = len(self._gold_medal_flags) - true_count
                    table = wandb.Table(data=[["Gold Medal", true_count], ["No Gold Medal", false_count]], columns=["label", "count"])
                    current_log_data["plots/gold_medal_bar_cumulative"] = wandb.plot.bar(table, "label", "count", title="Gold Medal Steps (Cumulative)")
                if self._silver_medal_flags:
                    true_count = sum(self._silver_medal_flags); false_count = len(self._silver_medal_flags) - true_count
                    table = wandb.Table(data=[["Silver Medal", true_count], ["No Silver Medal", false_count]], columns=["label", "count"])
                    current_log_data["plots/silver_medal_bar_cumulative"] = wandb.plot.bar(table, "label", "count", title="Silver Medal Steps (Cumulative)")
                if self._bronze_medal_flags:
                    true_count = sum(self._bronze_medal_flags); false_count = len(self._bronze_medal_flags) - true_count
                    table = wandb.Table(data=[["Bronze Medal", true_count], ["No Bronze Medal", false_count]], columns=["label", "count"])
                    current_log_data["plots/bronze_medal_bar_cumulative"] = wandb.plot.bar(table, "label", "count", title="Bronze Medal Steps (Cumulative)")

            # Metric History Scatter
            if self._metric_hist: 
                valid_metrics_for_plot = [m for m in self._metric_hist if isinstance(m, (int, float)) and not pd.isna(m)]
                if valid_metrics_for_plot:
                    metric_scatter_data = [[i, m] for i, m in enumerate(valid_metrics_for_plot)]
                    if metric_scatter_data:
                         tbl = wandb.Table(data=metric_scatter_data, columns=["step_idx_non_buggy", "metric_value"])
                         current_log_data["plots/val_metric_history_scatter"] = wandb.plot.scatter(tbl, "step_idx_non_buggy", "metric_value", title="Validation Metric History (Non-Buggy Steps)")
            
            # also keep a local copy so we can write a history.csv
            self._step_logs.append(current_log_data.copy())
            self.wandb_run.log(current_log_data, step=current_step_number)
            logger.debug(f"W&B: Logged step {current_step_number} data. Keys: {list(current_log_data.keys())}", extra={"verbose": True})
        except Exception as e:
            logger.error(f"W&B: Error logging step data for step {current_step_number}: {e}", exc_info=True)

    def _stage_files_for_wandb_artifacts(self):
        """Copies necessary files to a temporary staging area for W&B artifact creation."""
        logs_exp_dir = Path("logs") / self.cfg.exp_name
        workspaces_exp_dir = self.cfg.workspace_dir 
        
        # This is the staging directory *within* the run's log folder.
        # W&B artifacts will be created FROM this staging directory.
        wandb_artifacts_staging_dir = logs_exp_dir / "wandb_artifacts_final" 
        if wandb_artifacts_staging_dir.exists():
            shutil.rmtree(wandb_artifacts_staging_dir) 
        wandb_artifacts_staging_dir.mkdir(parents=True, exist_ok=True)

        # 1. Journal: Copied from logs/{exp_name}/journal.json
        journal_src = logs_exp_dir / "journal.json"
        if journal_src.exists():
            shutil.copy2(journal_src, wandb_artifacts_staging_dir / "journal.json")
            logger.info(f"W&B Staging: Copied journal.json to {wandb_artifacts_staging_dir}")

        # 2. Best Solution folder: Copied from workspaces/{exp_name}/best_solution
        best_solution_src_ws = workspaces_exp_dir / "best_solution"
        best_solution_dst_stage = wandb_artifacts_staging_dir / "best_solution_code" # Subdir in staging
        if best_solution_src_ws.exists() and any(best_solution_src_ws.iterdir()):
            shutil.copytree(best_solution_src_ws, best_solution_dst_stage, dirs_exist_ok=True)
            logger.info(f"W&B Staging: Copied best_solution folder to {best_solution_dst_stage}")
        
        # 3. Best Submission folder: Copied from workspaces/{exp_name}/best_submission
        best_submission_src_ws = workspaces_exp_dir / "best_submission"
        best_submission_dst_stage = wandb_artifacts_staging_dir / "best_submission_file" # Subdir in staging
        if best_submission_src_ws.exists() and any(best_submission_src_ws.iterdir()):
            shutil.copytree(best_submission_src_ws, best_submission_dst_stage, dirs_exist_ok=True)
            # shutil.copy2(best_submission_src_ws / "submission.csv", best_submission_dst_stage / "submission.csv")
            logger.info(f"W&B Staging: Copied best_submission folder to {best_submission_dst_stage}")
        
        # 4. Submission History per step: Copied from logs/{exp_name}/submission_history_per_step
        submission_history_src = self.submission_history_dir 
        submission_history_dst_stage = wandb_artifacts_staging_dir / "all_step_submissions_history"
        if submission_history_src.exists() and any(submission_history_src.iterdir()):
            shutil.copytree(submission_history_src, submission_history_dst_stage, dirs_exist_ok=True)
            logger.info(f"W&B Staging: Copied submission_history_per_step to {submission_history_dst_stage}")
        
        return wandb_artifacts_staging_dir


    def finalize_run(self, journal: Journal): # Removed competition_benchmarks from args, use self.
        if not self.wandb_run:
            self.app_logger.info("W&B run not available, skipping finalization.")
            return

        self.app_logger.info("W&B: Finalizing run...")
        try:
            # --- Calculate and Log Summary Metrics ---
            summary_data = {}
            wo_step = None; num_working_code_steps = 0; buggy_nodes_count = 0
            total_code_quality_non_buggy = 0; count_code_quality_non_buggy = 0
            
            for node in journal.nodes: # Iterate through the final journal
                if not node.is_buggy:
                    num_working_code_steps +=1
                    if wo_step is None: wo_step = node.step 
                    if node.code_quality is not None:
                        total_code_quality_non_buggy += node.code_quality
                        count_code_quality_non_buggy +=1
                else:
                    buggy_nodes_count += 1
            
            summary_data["summary/steps_to_first_working_code"] = (wo_step + 1) if wo_step is not None else (self.cfg.agent.steps + 10)
            summary_data["summary/num_working_code_steps_total"] = num_working_code_steps # Total non-buggy steps
            summary_data["summary/num_buggy_nodes_total"] = buggy_nodes_count
            summary_data["summary/avg_code_quality_non_buggy_nodes"] = (total_code_quality_non_buggy / count_code_quality_non_buggy) if count_code_quality_non_buggy > 0 else 0
            
            if self.competition_benchmarks: # Use self.competition_benchmarks
                summary_data["summary/total_gold_medals_achieved"] = sum(self._gold_medal_flags)
                summary_data["summary/total_silver_medals_achieved"] = sum(self._silver_medal_flags)
                summary_data["summary/total_bronze_medals_achieved"] = sum(self._bronze_medal_flags)
                summary_data["summary/total_steps_above_median"] = sum(self._above_median_flags)
            
            summary_data["summary/total_effective_fixes_in_run"] = sum(self._effective_fix_flags)
            summary_data["summary/total_effective_reflection_fixes_in_run"] = sum(self._effective_reflection_flags)
            summary_data["summary/total_submissions_by_non_buggy_nodes"] = sum(self._sub_produced_flags)

            best_node = journal.get_best_node(only_good=True) 
            if best_node and best_node.metric and best_node.metric.value is not None:
                summary_data["summary/best_validation_metric_overall"] = best_node.metric.value
                summary_data["summary/best_node_id_overall"] = best_node.id
                summary_data["summary/best_node_step_overall"] = best_node.step + 1

                best_node_submission_path_ws = self.cfg.workspace_dir / "best_submission" / "submission.csv"
                summary_data["summary/best_overall_solution_produced_submission_csv"] = 1 if best_node_submission_path_ws.exists() else 0
                
            self.wandb_run.summary.update(summary_data)
            self.app_logger.info(f"W&B: Updated run summary with final aggregations: {summary_data}")

            # --- Staging and Artifact Logging ---
            wandb_artifacts_staging_dir = self._stage_files_for_wandb_artifacts()
            sanitized_exp_name = self._sanitize_artifact_name_component(self.cfg.exp_name)

            # Log Journal as an artifact
            if (wandb_artifacts_staging_dir / "journal.json").exists():
                artifact_journal = wandb.Artifact(f"{sanitized_exp_name}_run_journal", type="run-journal")
                artifact_journal.add_file(str(wandb_artifacts_staging_dir / "journal.json"))
                self.wandb_run.log_artifact(artifact_journal)
                self.app_logger.info(f"W&B: Logged run_journal artifact.")

            # Log Best Solution Code package as an artifact
            best_solution_pkg_path = wandb_artifacts_staging_dir / "best_solution_code"
            if best_solution_pkg_path.exists() and any(best_solution_pkg_path.iterdir()):
                artifact_code = wandb.Artifact(f"{sanitized_exp_name}_best_solution_package", type="solution-package")
                artifact_code.add_dir(str(best_solution_pkg_path))
                self.wandb_run.log_artifact(artifact_code)
                self.app_logger.info(f"W&B: Logged best_solution_package artifact.")
            
            # Log Best Submission File(s) package as an artifact
            best_submission_pkg_path = wandb_artifacts_staging_dir / "best_submission_file" # Name matches staging
            if best_submission_pkg_path.exists() and any(best_submission_pkg_path.iterdir()):
                artifact_submission = wandb.Artifact(f"{sanitized_exp_name}_best_submission_package", type="submission-package")
                artifact_submission.add_dir(str(best_submission_pkg_path)) 
                self.wandb_run.log_artifact(artifact_submission)
                self.app_logger.info(f"W&B: Logged best_submission_package artifact.")
            
            # Log Submission History (all CSVs from non-buggy steps) as an artifact
            all_submissions_history_path = wandb_artifacts_staging_dir / "all_step_submissions_history"
            if all_submissions_history_path.exists() and any(all_submissions_history_path.iterdir()):
                artifact_all_subs = wandb.Artifact(f"{sanitized_exp_name}_all_step_submissions", type="all-submissions")
                artifact_all_subs.add_dir(str(all_submissions_history_path))
                self.wandb_run.log_artifact(artifact_all_subs)
                self.app_logger.info(f"W&B: Logged all_step_submissions artifact.")

            # Main log files for the run
            for log_file_name in ["aide.log", "aide.verbose.log", "final_wandb_summary.csv"]: # Add summary CSV
                log_file_path = self.cfg.log_dir / log_file_name # Path is logs/{exp_name}/file.log
                if log_file_path.exists():
                    # To make it appear as "exp_name/file.log" in W&B files tab
                    self.wandb_run.save(str(log_file_path), base_path=str(self.cfg.log_dir.parent))
                    self.app_logger.info(f"W&B: Saved direct file: {log_file_path.name}")
            
            # Other generated reports from logs/{exp_name}
            reports_to_save_directly = ["calculated_metrics_results.json", "advanced_metrics.json", "report.md", "history.csv", "tree_plot.html"]
            for report_file_name in reports_to_save_directly:
                file_path = self.cfg.log_dir / report_file_name 
                if file_path.exists():
                    self.wandb_run.save(str(file_path), base_path=str(self.cfg.log_dir.parent))
                    self.app_logger.info(f"W&B: Saved direct report file: {file_path.name}")
            
            # This can be an alternative or addition to specific artifact logging.
            self.app_logger.info(f"W&B: Saving all files from log directory: {self.cfg.log_dir}")
            self.wandb_run.save(str(self.cfg.log_dir / "*"), base_path=str(self.cfg.log_dir.parent))

            local_report = self.cfg.log_dir / "wandb_summary_metrics.json"
            with open(local_report, "w") as f:
                json.dump(summary_data, f, indent=2)
             # ─── DUMP LOCAL HISTORY ─────────────────────────────────────────

            history_path = Path(self.cfg.log_dir) / "history.csv"
            pd.DataFrame(self._step_logs).to_csv(history_path, index=False)
            self.app_logger.info(f"Wrote local history.csv → {history_path}")

        except Exception as e:
            self.app_logger.error(f"W&B: Error during finalize_run: {e}", exc_info=True)
        finally:
            if self.wandb_run: 
                save_logs_to_wandb()
                self.wandb_run.finish()
                self.app_logger.info("W&B run finished.")

