# START OF NEW FILE aide-ds/finetuning/README.md
# AIDE Finetuning Pipeline

This directory contains scripts to prepare data generated by AIDE runs and fine-tune language models (specifically DeepSeek-7B using QLoRA in this example) on that data.

## 1. Prepare Data (`prepare_data.py`)

This script converts the `journal.json` files from multiple AIDE experiments into a JSONL format suitable for instruction fine-tuning.

**Usage:**

```bash
python prepare_data.py <path_to_aide_log_directory> <output_jsonl_file> [--task_desc_file <path_to_task_desc>]


<path_to_aide_log_directory>: The directory containing subfolders for each AIDE experiment (e.g., aide-ds/logs/). Each subfolder should contain a journal.json file.
<output_jsonl_file>: The path where the formatted JSONL data will be saved (e.g., finetuning_data.jsonl).
--task_desc_file (Optional): If all journals correspond to the same task, provide the path to the task description file (e.g., aide-ds/example_tasks/house_prices.md). If omitted, the script will try to find the description from each experiment's config.yaml.

Output Format:
The script generates a JSONL file where each line is a JSON object. By default, it uses a format compatible with trl.SFTTrainer's dataset_text_field="text" which combines instruction, input, and output into a single string field, resembling the Alpaca format. It also includes separate fields for potential use with other trainers:
{"text": "Below is an instruction... ### Instruction:\n...\n\n### Input:\n...\n\n### Response:\n...", "instruction": "...", "input": "...", "output": "..."}
{"text": "...", "instruction": "...", "input": "...", "output": "..."}

pip install -r requirements-finetune.txt
---


Okay, I will perform the requested modifications. This involves creating new backend files, updating existing ones, adapting prompts, and generating the fine-tuning scripts.

Here are the code scripts for the new and modified files:

**1. New Backend: DeepSeek via vLLM**

```python
# START OF FILE aide-ds/aide/backend/backend_vllm.py
"""Backend for vLLM OpenAI-compatible API."""

import json
import logging
import time
import os
from funcy import notnone, once, select_values
import openai
from omegaconf import OmegaConf # To read config if needed

from aide.backend.utils import (FunctionSpec, OutputType, opt_messages_to_list, backoff_create)

logger = logging.getLogger("aide")

_client: openai.OpenAI = None  # type: ignore
_vllm_config: dict = {} # To store VLLM specific config

# Define potential exceptions (adjust if vLLM raises different ones)
VLLM_API_EXCEPTIONS = (
    openai.APIConnectionError,
    openai.RateLimitError,
    openai.APITimeoutError,
    openai.APIError,
    openai.InternalServerError
)

@once
def _setup_vllm_client():
    """Sets up the OpenAI client to point to the vLLM server."""
    global _client, _vllm_config
    try:
        # Attempt to load config if not already loaded (may need adjustment based on how config is passed)
        if not _vllm_config:
             # This assumes config is accessible globally or passed differently.
             # A cleaner way might be to pass cfg during the first query.
             # For now, we rely on environment variables or a hardcoded default.
             _vllm_config['base_url'] = os.getenv("VLLM_BASE_URL", "http://localhost:8000/v1")
             _vllm_config['api_key'] = os.getenv("VLLM_API_KEY", "EMPTY") # vLLM often uses 'EMPTY' or no key

        logger.info(f"Setting up vLLM client with base_url: {_vllm_config['base_url']}")
        _client = openai.OpenAI(
            base_url=_vllm_config['base_url'],
            api_key=_vllm_config['api_key'],
            max_retries=0 # Rely on backoff decorator
        )
    except Exception as e:
        logger.error(f"Failed to setup vLLM client: {e}", exc_info=True)
        raise

def set_vllm_config(cfg: OmegaConf):
    """Allows setting vLLM config explicitly, e.g., from the main run."""
    global _vllm_config
    if cfg.get('vllm'):
         _vllm_config['base_url'] = cfg.vllm.get('base_url', "http://localhost:8000/v1")
         _vllm_config['api_key'] = cfg.vllm.get('api_key', "EMPTY")
    else: # Fallback to environment or defaults if 'vllm' section is missing
         _vllm_config['base_url'] = os.getenv("VLLM_BASE_URL", "http://localhost:8000/v1")
         _vllm_config['api_key'] = os.getenv("VLLM_API_KEY", "EMPTY")


def query(
    system_message: str | None,
    user_message: str | None,
    func_spec: FunctionSpec | None = None,
    convert_system_to_user: bool = False,
    **model_kwargs,
) -> tuple[OutputType, float, int, int, dict]:
    """
    Query a model served via vLLM using the OpenAI-compatible endpoint.
    """
    _setup_vllm_client() # Ensure client is initialized

    # Filter out None kwargs and prepare messages
    filtered_kwargs: dict = select_values(notnone, model_kwargs)
    messages = opt_messages_to_list(system_message, user_message, convert_system_to_user=convert_system_to_user)

    # Handle function calling specification if provided
    tools_arg = None
    tool_choice_arg = None
    if func_spec is not None:
        # Check if the served model likely supports tools (heuristic)
        model_name = filtered_kwargs.get("model", "")
        if "instruct" in model_name.lower() or "tool" in model_name.lower(): # Basic check
            tools_arg = [{"type": "function", "function": func_spec.to_dict()}]
            # Forcing function call might depend on exact vLLM/model support.
            # Standard OpenAI forcing: {"type": "function", "function": {"name": func_spec.name}}
            # Some vLLM setups might need just "auto" or the specific name string. Test this.
            tool_choice_arg = {"type": "function", "function": {"name": func_spec.name}}
            logger.info("vLLM: Function spec provided, attempting tool call.")
        else:
            logger.warning("vLLM: Function spec provided, but model name doesn't suggest tool support. Skipping tool call.")
            func_spec = None # Disable func_spec if unsure

    # Add tools/tool_choice to kwargs if applicable
    if tools_arg:
        filtered_kwargs["tools"] = tools_arg
    if tool_choice_arg:
        filtered_kwargs["tool_choice"] = tool_choice_arg

    # Perform the API call with backoff
    t0 = time.time()
    try:
        completion = backoff_create(
            _client.chat.completions.create,
            VLLM_API_EXCEPTIONS,
            messages=messages,
            **filtered_kwargs,
        )
    except Exception as e:
         logger.error(f"vLLM query failed: {e}", exc_info=True)
         # Re-raise or return an error state if desired
         raise # Re-raise for now

    req_time = time.time() - t0

    # Process the response
    choice = completion.choices[0]
    message = choice.message

    output: OutputType
    if func_spec and message.tool_calls:
        # Process function call
        tool_call = message.tool_calls[0]
        if tool_call.function.name == func_spec.name:
            try:
                output = json.loads(tool_call.function.arguments)
                logger.info(f"vLLM: Successfully parsed tool call arguments for '{func_spec.name}'.")
            except json.JSONDecodeError as e:
                logger.error(f"vLLM: Error decoding JSON arguments for tool '{func_spec.name}': {tool_call.function.arguments}", exc_info=True)
                # Fallback to raw text or raise error? Let's return raw args for debugging.
                output = tool_call.function.arguments # Or raise e
        else:
            logger.error(f"vLLM: Tool call name mismatch. Expected '{func_spec.name}', got '{tool_call.function.name}'.")
            output = message.content or "" # Fallback to text content
    else:
        # Process regular text response
        output = message.content or ""

    # Extract token counts and other info (may vary with vLLM versions)
    input_tokens = completion.usage.prompt_tokens if completion.usage else 0
    output_tokens = completion.usage.completion_tokens if completion.usage else 0

    info = {
        "model": completion.model,
        "finish_reason": choice.finish_reason,
        # Add other relevant info if available from vLLM completion object
    }

    return output, req_time, input_tokens, output_tokens, info

# END OF FILE aide-ds/aide/backend/backend_vllm.py
```

**2. Modified Backend Dispatch**

```python
# START OF MODIFIED FILE aide-ds/aide/backend/__init__.py
import logging
from . import backend_anthropic, backend_local, backend_openai, backend_openrouter, backend_gdm, backend_deepseek, backend_vllm # Added backend_vllm
from .utils import FunctionSpec, OutputType, PromptType, compile_prompt_to_md

logger = logging.getLogger("aide")


def determine_provider(model: str) -> str:
    if model.startswith("gpt-") or model.startswith("o1-") or model.startswith("o3-"):
        return "openai"
    elif model.startswith("claude-"):
        return "anthropic"
    elif model.startswith("deepseek"): # Assuming direct API or a non-vLLM/non-local setup
        return "deepseek"
    elif model.startswith("gemini-"):
        return "gdm"
    elif model.startswith("vllm/"): # New condition for vLLM
        return "vllm"
    # all other models are handle by local (Hugging Face Transformers)
    else:
        return "local"


provider_to_query_func = {
    "openai": backend_openai.query,
    "anthropic": backend_anthropic.query,
    "gdm": backend_gdm.query,
    "openrouter": backend_openrouter.query,
    "deepseek": backend_deepseek.query,
    "vllm": backend_vllm.query, # Added vLLM mapping
    "local": backend_local.query,
}


def query(
    system_message: PromptType | None,
    user_message: PromptType | None,
    model: str,
    temperature: float | None = None,
    max_tokens: int | None = None,
    func_spec: FunctionSpec | None = None,
    convert_system_to_user: bool = False,
    # local_use : bool = False, # This parameter seems unused, consider removing
    reasoning_effort: str | None = None,
    # **model_kwargs, # Changed to pass all extra kwargs
    **kwargs, # Use kwargs to capture all extra arguments for flexibility
) -> OutputType:
    """
    General LLM query for various backends with a single system and user message.
    Supports function calling for some backends.

    Args:
        system_message (PromptType | None): Uncompiled system message.
        user_message (PromptType | None): Uncompiled user message.
        model (str): string identifier for the model to use (e.g., "gpt-4-turbo", "vllm/deepseek-coder", "codellama/CodeLlama-7b-Instruct-hf").
        temperature (float | None, optional): Temperature to sample at.
        max_tokens (int | None, optional): Maximum number of tokens to generate.
        func_spec (FunctionSpec | None, optional): Optional FunctionSpec object defining a function call.
        convert_system_to_user (bool): Flag to convert system message role to user.
        reasoning_effort (str | None): Specific parameter for some models (like o3-mini).
        **kwargs: Additional keyword arguments passed directly to the backend query function (e.g., `hf_local`, `vllm` configs, `top_p`).

    Returns:
        OutputType: A string completion if func_spec is None, otherwise a dict with the function call details.
    """

    provider = determine_provider(model)
    logger.info(f"Determined provider: {provider} for model: {model}")

    model_query_kwargs = {
        "model": model, # Pass the specific model identifier
        "convert_system_to_user": convert_system_to_user,
        **kwargs # Pass through all other kwargs
    }

    # Handle standard parameters, allowing backend-specific ones to override if needed
    if temperature is not None:
        model_query_kwargs.setdefault("temperature", temperature)
    if max_tokens is not None:
        # Use provider-specific naming conventions if necessary, otherwise default
        if provider == "openai" and model.startswith("o3-"):
            model_query_kwargs.setdefault("max_completion_tokens", max_tokens)
        else:
            model_query_kwargs.setdefault("max_tokens", max_tokens)
            
    if provider == "openai" and model.startswith("o3-") and reasoning_effort:
         model_query_kwargs.setdefault("reasoning_effort", reasoning_effort)

    # Compile prompts
    compiled_system_message = compile_prompt_to_md(system_message) if system_message else None
    compiled_user_message = compile_prompt_to_md(user_message) if user_message else None

    # Log query details
    logger.info("---Querying model---", extra={"verbose": True})
    if compiled_system_message:
        logger.info(f"System: {compiled_system_message}", extra={"verbose": True})
    if compiled_user_message:
        logger.info(f"User: {compiled_user_message}", extra={"verbose": True})
    if func_spec:
        logger.info(f"Function spec: {func_spec.to_dict()}", extra={"verbose": True})
    logger.info(f"Query Kwargs: {model_query_kwargs}", extra={"verbose": True})


    # Get and call the appropriate backend function
    query_func = provider_to_query_func.get(provider)
    if query_func is None:
        raise ValueError(f"Unsupported provider for model: {model}")

    output, req_time, in_tok_count, out_tok_count, info = query_func(
        system_message=compiled_system_message,
        user_message=compiled_user_message,
        func_spec=func_spec,
        **model_query_kwargs,
    )

    # Log response details
    logger.info(f"Response Raw: {output}", extra={"verbose": True})
    logger.info(f"Time: {req_time:.2f}s, InTokens: {in_tok_count}, OutTokens: {out_tok_count}", extra={"verbose": True})
    logger.info(f"Info: {info}", extra={"verbose": True})
    logger.info(f"---Query complete---", extra={"verbose": True})

    return output

# END OF MODIFIED FILE aide-ds/aide/backend/__init__.py
```

**3. Enhanced Backend: Hugging Face Local (`backend_local.py`)**

```python
# START OF MODIFIED FILE aide-ds/aide/backend/backend_local.py
import logging
import time
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
import torch
from funcy import notnone, select_values

from aide.backend.utils import FunctionSpec, OutputType, opt_messages_to_list

logger = logging.getLogger("aide")

# Using a dictionary cache to support multiple models concurrently
class LocalLLMCache:
    _models_tokenizers = {}
    _pipelines = {} # Cache for pipelines if needed later

    @classmethod
    def get_model_tokenizer(cls, model_name: str, load_in_4bit: bool = True):
        """Loads model and tokenizer, caching them."""
        if model_name not in cls._models_tokenizers:
            logger.info(f"Loading local model and tokenizer: {model_name}")
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                if tokenizer.pad_token is None:
                    # Common practice: set pad token to eos token if not defined
                    tokenizer.pad_token = tokenizer.eos_token
                    logger.info(f"Tokenizer pad_token set to eos_token: {tokenizer.eos_token}")

                quantization_config = None
                if load_in_4bit and torch.cuda.is_available():
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.bfloat16, # Changed to bfloat16 for better compatibility
                        bnb_4bit_use_double_quant=True,
                        bnb_4bit_quant_type="nf4",
                    )
                    logger.info("Using 4-bit quantization (BitsAndBytesConfig).")
                elif load_in_4bit:
                    logger.warning("CUDA not available, cannot load in 4-bit. Loading in default precision.")

                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    quantization_config=quantization_config,
                    device_map="auto", # Let accelerate handle device placement
                    torch_dtype=torch.bfloat16 if quantization_config else None, # Match compute dtype
                    trust_remote_code=True, # Required for some models
                )
                logger.info(f"Local model '{model_name}' loaded successfully.")
                cls._models_tokenizers[model_name] = (tokenizer, model)
            except Exception as e:
                logger.exception(f"Failed to load local model {model_name}")
                raise
        return cls._models_tokenizers[model_name]

def generate_response(
    model_name: str,
    prompt_text: str, # Changed name for clarity
    temperature: float = 1.0,
    max_new_tokens: int = 1500, # Renamed from max_tokens
    top_p: float | None = None,
    top_k: int | None = None,
    repetition_penalty: float | None = None,
    load_in_4bit: bool = True,
    **gen_kwargs, # Capture other generation kwargs
):
    """Generates response using the loaded local model."""
    try:
        tokenizer, model = LocalLLMCache.get_model_tokenizer(model_name, load_in_4bit)

        # Encode the prompt
        inputs = tokenizer(prompt_text, return_tensors="pt", return_attention_mask=True)
        input_ids = inputs["input_ids"].to(model.device) # Move inputs to model's device
        attention_mask = inputs["attention_mask"].to(model.device)

        # Prepare generation arguments, filtering out None values
        generation_config = select_values(notnone, {
            "temperature": temperature,
            "max_new_tokens": max_new_tokens,
            "top_p": top_p,
            "top_k": top_k,
            "repetition_penalty": repetition_penalty,
            "pad_token_id": tokenizer.eos_token_id, # Important for open-ended generation
            "eos_token_id": tokenizer.eos_token_id,
            "do_sample": temperature > 0.0, # Sample only if temperature is positive
            **gen_kwargs, # Include any other passed arguments
        })
        
        logger.info(f"Generating response with config: {generation_config}")

        # Generate
        with torch.no_grad(): # Inference mode
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                **generation_config
            )

        # Decode, skipping special tokens and the prompt
        # Need to handle potential differences in output structure/indexing
        # This assumes output contains the prompt; we slice it off.
        output_text = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)

        return output_text.strip()
    except Exception as e:
        logger.exception("Error generating response with local model")
        # Depending on desired behavior, could return None, empty string, or re-raise
        return f"Error during generation: {e}"


def query(
    system_message: str | None,
    user_message: str | None,
    model: str, # This is the model_name for HF
    temperature: float | None = None,
    max_tokens: int | None = None, # Corresponds to max_new_tokens
    func_spec: FunctionSpec | None = None, # Local models generally don't support this well yet
    convert_system_to_user: bool = False, # Less relevant if using chat templates
    hf_local: dict | None = None, # Specific config for HF local
    **kwargs, # Capture any remaining kwargs like top_p, top_k, etc.
) -> tuple[OutputType, float, int, int, dict]:
    """Query a local Hugging Face model."""
    if func_spec is not None:
         logger.warning("Function calling (func_spec) is generally not supported by local HF models via this backend. Ignoring.")

    # Process hf_local config or defaults
    hf_config = hf_local or {}
    use_chat_template = hf_config.get('use_chat_template', True) # Default to using chat template
    load_in_4bit = hf_config.get('load_in_4bit', True) # Default to 4-bit if possible

    # Prepare generation parameters
    gen_params = {
        "temperature": temperature if temperature is not None else 1.0,
        "max_new_tokens": max_tokens if max_tokens is not None else 1500,
        "top_p": kwargs.get('top_p', hf_config.get('top_p')),
        "top_k": kwargs.get('top_k', hf_config.get('top_k')),
        "repetition_penalty": kwargs.get('repetition_penalty', hf_config.get('repetition_penalty')),
        "load_in_4bit": load_in_4bit,
    }

    # Construct the prompt text
    prompt_input_text: str
    if use_chat_template:
        # Prepare messages for chat template
        messages = []
        if system_message:
             # If converting or if model prefers user role for system prompt
             if convert_system_to_user or hf_config.get('system_as_user', False):
                 messages.append({"role": "user", "content": system_message})
             else:
                 messages.append({"role": "system", "content": system_message})
        if user_message:
            messages.append({"role": "user", "content": user_message})

        # Load only the tokenizer to apply the template
        try:
            tokenizer, _ = LocalLLMCache.get_model_tokenizer(model, load_in_4bit=False) # Don't need full model yet
            prompt_input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            logger.info("Applied chat template.")
        except Exception as e:
            logger.warning(f"Failed to apply chat template for {model}: {e}. Falling back to simple concatenation.")
            # Fallback to simple concatenation
            prompt_parts = [msg for msg in [system_message, user_message] if msg]
            prompt_input_text = "\n\n".join(prompt_parts)
    else:
        # Simple concatenation if chat template is disabled
        prompt_parts = [msg for msg in [system_message, user_message] if msg]
        prompt_input_text = "\n\n".join(prompt_parts)

    logger.info(f"Final prompt text:\n{prompt_input_text[:500]}...") # Log start of prompt

    # Generate response
    t0 = time.time()
    response_text = generate_response(
        model_name=model,
        prompt_text=prompt_input_text,
        **gen_params
    )
    req_time = time.time() - t0

    # For local models, token counts are often not easily available or accurate without extra steps
    # Return 0 for now, or implement token counting if crucial
    in_tokens = 0
    out_tokens = 0
    info = {"model_name": model, "provider": "hf_local"}

    logger.info(f"Local response: {response_text[:500]}...")
    return response_text, req_time, in_tokens, out_tokens, info

# END OF MODIFIED FILE aide-ds/aide/backend/backend_local.py
```

**4. Modified Configuration**

```yaml
# START OF MODIFIED FILE aide-ds/aide/utils/config.yaml

# path to the task data directory
data_dir: null

# either provide a path to a plaintext file describing the task
desc_file: null
# or provide the task goal (and optionally evaluation information) as arguments
goal: null
eval: null

log_dir: logs
log_level: INFO
workspace_dir: workspaces

# whether to unzip any archives in the data directory
preprocess_data: True
# whether to copy the data to the workspace directory (otherwise it will be symlinked)
# copying is recommended to prevent the agent from accidentally modifying the original data
copy_data: True

exp_name: null # a random experiment name will be generated if not provided

# settings for code execution
exec:
  timeout: 3600
  agent_file_name: runfile.py
  format_tb_ipython: False

# agent hyperparams
agent:
  # how many improvement iterations to run
  steps: 20
  # total time available to agent
  time_limit: 3600 # 1 hour (purely informational)
  # whether to instruct the agent to use CV (set to 1 to disable)
  k_fold_validation: 5
  # whether to instruct the agent to generate a prediction function
  expose_prediction: False
  # whether to provide the agent with a preview of the data
  data_preview: True
  # whether to convert system messages into user messages (can be useful for some local models)
  convert_system_to_user: False
  # whether to obfuscate that we're doing kaggle
  obfuscate: False
  # Style of prompts to use: "default" (optimized for large models), "simple" (better for smaller OS models)
  prompt_style: "simple" # CHANGED default to simple

  # LLM settings for coding
  code:
    # Examples:
    # model: "vllm/deepseek-ai/deepseek-coder-7b-instruct-v1.5" # If using vLLM backend
    # model: "deepseek-ai/deepseek-coder-7b-instruct-v1.5" # If using local HF backend
    model: "vllm/deepseek-ai/deepseek-coder-7b-instruct-v1.5" # CHANGED: Example using vLLM
    temp: 0.2

  # LLM settings for evaluating program output / tracebacks
  feedback:
    # Use a capable (possibly closed-source) model for feedback, or a local one if preferred.
    # model: "gpt-4-turbo"
    model: "o3-mini" # Keeping o3-mini as a capable small model example
    # model: "vllm/deepseek-ai/deepseek-coder-7b-instruct-v1.5" # Can also use the same model
    temp: 0.5

  # hyperparameters for the tree search
  search:
    max_debug_depth: 5
    debug_prob: 0.7
    num_drafts: 7

# Configuration for vLLM backend
vllm:
  base_url: "http://localhost:8000/v1" # Default vLLM OpenAI-compatible endpoint
  api_key: "EMPTY" # Usually not needed or dummy

# Configuration for local Hugging Face backend
hf_local:
  use_chat_template: true # Apply model's chat template if available
  load_in_4bit: true      # Attempt 4-bit quantization if CUDA is available
  # Add other generation params if needed:
  # top_p: 0.9
  # top_k: 50
  # repetition_penalty: 1.1

# END OF MODIFIED FILE aide-ds/aide/utils/config.yaml
```

**5. Modified Agent (`agent.py`) with Simpler Prompts and Few-Shot Reflection**

```python
# START OF MODIFIED FILE aide-ds/aide/agent.py

import shutil
import logging
import random
import time
from typing import Any, Callable, cast

import humanize
from .backend import FunctionSpec, compile_prompt_to_md, query
from .interpreter import ExecutionResult
from .journal import Journal, Node
from .utils import data_preview
from .utils.config import Config
from .utils.metric import MetricValue, WorstMetricValue
from .utils.response import extract_code, extract_text_up_to_code, wrap_code
# Import the few-shot reflection function
from .utils.self_reflection import perform_two_step_reflection_with_fewshot

logger = logging.getLogger("aide")


def format_time(time_in_sec: int):
    return f"{time_in_sec // 3600}hrs {(time_in_sec % 3600) // 60}mins {time_in_sec % 60}secs"


ExecCallbackType = Callable[[str, bool], ExecutionResult]

review_func_spec = FunctionSpec(
    name="submit_review",
    json_schema={
        "type": "object",
        "properties": {
            "is_bug": {
                "type": "boolean",
                "description": "true if the output log shows that the execution failed or has some bug, otherwise false.",
            },
            "has_csv_submission": {
                "type": "boolean",
                "description": "true if the code saves the predictions on the test data"
                " in a `submission.csv` file in the `./submission/` directory, otherwise false."
                " Note that the file MUST be saved in the ./submission/ directory for this to be evaluated as true."
                " Otherwise, it should be evaluated as false."
                " You can assume the ./submission/ directory exists and is writable.",
            },
            "summary": {
                "type": "string",
                "description": "write a short summary (1-2 sentences) describing "
                " the empirical findings (metric value). Alternatively mention if there is a bug or"
                " the submission.csv was not properly produced."
                " DO NOT suggest fixes or improvements.", # Simplified
            },
            "metric": {
                "type": "number",
                "description": "If the code ran successfully and printed a metric, report the value of the validation metric. Otherwise, leave it null.", # Simplified
            },
            "lower_is_better": {
                "type": "boolean",
                "description": "true if the metric should be minimized (e.g., MSE, RMSE), false if the metric should be maximized (e.g., accuracy, F1).", # Simplified
            },
        },
        "required": [
            "is_bug",
            "has_csv_submission",
            "summary",
            "metric",
            "lower_is_better",
        ],
    },
    description="Submit a review evaluating the output of the training script.",
)


class Agent:
    def __init__(
        self,
        task_desc: str,
        cfg: Config,
        journal: Journal,
    ):
        super().__init__()
        self.task_desc = task_desc
        self.cfg = cfg
        self.acfg = cfg.agent
        self.journal = journal
        self.data_preview: str | None = None
        self.start_time = time.time()
        self.current_step = 0
        # Determine prompt style based on config
        self.use_simple_prompts = self.acfg.prompt_style == "simple"
        logger.info(f"Using prompt style: {'simple' if self.use_simple_prompts else 'default'}")

    def search_policy(self) -> Node | None:
        """Select a node to work on (or None to draft a new node)."""
        search_cfg = self.acfg.search

        # initial drafting
        if len(self.journal.draft_nodes) < search_cfg.num_drafts:
            logger.info("[search policy] drafting new node (not enough drafts)")
            return None

        # debugging
        if random.random() < search_cfg.debug_prob:
            # nodes that are buggy + leaf nodes + debug depth < max debug depth
            debuggable_nodes = [
                n
                for n in self.journal.buggy_nodes
                if (n.is_leaf and n.debug_depth <= search_cfg.max_debug_depth)
            ]
            if debuggable_nodes:
                node_to_debug = random.choice(debuggable_nodes)
                logger.info(f"[search policy] debugging node {node_to_debug.id}")
                return node_to_debug

        # back to drafting if no nodes to improve
        good_nodes = self.journal.good_nodes
        if not good_nodes:
            logger.info("[search policy] drafting new node (no good nodes)")
            return None

        # greedy
        greedy_node = self.journal.get_best_node()
        if greedy_node is None: # Should not happen if good_nodes is not empty, but for safety
             logger.warning("[search policy] No best node found despite having good nodes. Drafting.")
             return None
        logger.info(f"[search policy] greedy node selected: node {greedy_node.id}")
        return greedy_node

    @property
    def _prompt_environment(self):
        # Keep environment prompt simple, less crucial for OS models
        pkgs = ["numpy", "pandas", "scikit-learn", "xgboost", "lightgbm", "torch"]
        pkg_str = ", ".join([f"`{p}`" for p in pkgs])
        env_prompt = {
            "Installed Packages": f"You can use common ML packages like: {pkg_str}. Most standard packages are installed."
        }
        return env_prompt

    @property
    def _prompt_impl_guideline(self):
        tot_time_elapsed = time.time() - self.start_time
        tot_time_remaining = max(0, self.acfg.time_limit - tot_time_elapsed) # Ensure non-negative
        exec_timeout = int(min(self.cfg.exec.timeout, tot_time_remaining)) if tot_time_remaining > 0 else self.cfg.exec.timeout

        impl_guideline = [
            # Time/Step info might confuse smaller models, make it simpler or remove
            # f"<TOTAL_TIME_REMAINING: {format_time(tot_time_remaining)}>",
            # f"<TOTAL_STEPS_REMAINING: {self.acfg.steps - self.current_step}>",
            "**Code Requirements:**",
            "- Implement the solution described.",
            "- Print the validation metric value (e.g., using `print(f'Validation Metric: {metric_value}')`).",
            "- **CRITICAL: Save test predictions to `./submission/submission.csv`.** This exact path and filename is required.",
            "- Write a single, self-contained Python script.",
            "- Ensure the script runs to completion without errors.",
            "- Be mindful of execution time (limit: ~{humanize.naturaldelta(exec_timeout)}).",
            "- Input data is in the `./input/` directory.",
            "- Use `./working/` for temporary files if needed.",
            "- **REMEMBER: Output `./submission/submission.csv` is mandatory!**",
        ]
        if self.acfg.expose_prediction:
            # Keep simple for OS models
            impl_guideline.append(
                "- Include a `predict()` function for reuse on new data."
            )

        if self.acfg.k_fold_validation > 1:
             impl_guideline.append(
                 f"- If appropriate for the task, use {self.acfg.k_fold_validation}-fold cross-validation for evaluation."
             )

        return {"Implementation Guideline": impl_guideline}

    @property
    def _prompt_resp_fmt(self):
        # Make the format extremely explicit for OS models
        return {
            "Response Format": (
                "1. First, write a brief plan (3-5 sentences) outlining your solution approach.\n"
                "2. **Immediately after the plan**, start the Python code block like this:\n"
                "```python\n"
                "[YOUR PYTHON CODE HERE]\n"
                "```\n"
                "**IMPORTANT: There should be NO text after the final ```.**"
            )
        }

    def plan_and_code_query(self, prompt, retries=3) -> tuple[str, str]:
        """Generate a natural language plan + code in the same LLM call and split them apart."""
        completion_text = None
        for i in range(retries):
            try:
                 # Pass necessary config to the query function
                 completion_text = query(
                     system_message=prompt,
                     user_message=None,
                     model=self.acfg.code.model,
                     temperature=self.acfg.code.temp,
                     convert_system_to_user=self.acfg.convert_system_to_user,
                     # Pass relevant parts of config if needed by backends
                     hf_local=self.cfg.get('hf_local'),
                     vllm=self.cfg.get('vllm'),
                 )
                 code = extract_code(completion_text)
                 nl_text = extract_text_up_to_code(completion_text)

                 if code and nl_text:
                     logger.info("Plan and code extracted successfully.")
                     return nl_text, code
                 else:
                      logger.warning(f"Plan/Code extraction failed (Attempt {i+1}/{retries}). NL: {bool(nl_text)}, Code: {bool(code)}. Raw response: {completion_text[:200]}...")

            except Exception as e:
                 logger.error(f"Error during plan_and_code_query (Attempt {i+1}/{retries}): {e}", exc_info=True)
                 time.sleep(2) # Wait a bit before retrying after an error

        logger.error("Final plan + code extraction attempt failed. Returning empty plan and raw response as code.")
        # Return raw text as code if extraction fails completely
        return "", completion_text or ""


    def _draft(self) -> Node:
        if self.use_simple_prompts:
            introduction = (
                 "You are an expert Python programmer writing a machine learning solution. "
                 "Outline your plan briefly, then implement it in Python code."
            )
            solution_sketch_guideline = [
                 "Keep this first solution relatively simple.",
                 "Base your plan on the Task Description and Data Overview.",
                 "Describe your plan in 3-5 sentences.",
                 "Choose a reasonable evaluation metric if not specified.",
                 "Focus on model training and prediction generation.",
                 "Data is ready in `./input/`. No need to unzip.",
             ]
        else:
             # Original complex prompt
             introduction = (
                 "You are a Kaggle grandmaster attending a competition. "
                 "In order to win this competition, you need to come up with an excellent and creative plan "
                 "for a solution and then implement this solution in Python. We will now provide a description of the task."
             )
             if self.acfg.obfuscate:
                 introduction = (
                     "You are an expert machine learning engineer attempting a task. "
                     "In order to complete this task, you need to come up with an excellent and creative plan "
                     "for a solution and then implement this solution in Python. We will now provide a description of the task."
                 )
             solution_sketch_guideline = [
                 "This first solution design should be relatively simple, without ensembling or hyper-parameter optimization.",
                 "Take the Memory section into consideration when proposing the design,"
                 " don't propose the same modelling solution but keep the evaluation the same.",
                 "The solution sketch should be 3-5 sentences.",
                 "Propose an evaluation metric that is reasonable for this task.",
                 "Don't suggest to do EDA.",
                 "The data is already prepared and available in the `./input` directory. There is no need to unzip any files.",
             ]

        # Simplify Memory for OS models if needed (e.g., only last or best attempt)
        memory_summary = self.journal.generate_summary()
        if self.use_simple_prompts and len(self.journal.good_nodes) > 0:
             best_prev = self.journal.get_best_node()
             if best_prev:
                  memory_summary = f"Best Previous Attempt:\nDesign: {best_prev.plan}\nResults: {best_prev.analysis}\nValidation Metric: {best_prev.metric.value}\n"
             else:
                  memory_summary = "No successful previous attempts recorded."
        elif self.use_simple_prompts:
             memory_summary = "No previous attempts recorded yet."


        prompt: Any = {
            "Introduction": introduction,
            "Task Description": self.task_desc,
            # Use simplified memory for simple prompts
            "Memory / Previous Attempts": memory_summary,
            "Instructions": {},
        }
        prompt["Instructions"] |= self._prompt_resp_fmt
        prompt["Instructions"]["Solution Plan Guideline"] = solution_sketch_guideline
        prompt["Instructions"] |= self._prompt_impl_guideline
        prompt["Instructions"] |= self._prompt_environment

        if self.acfg.data_preview and self.data_preview:
            prompt["Data Overview"] = self.data_preview

        plan, code = self.plan_and_code_query(prompt)
        if not code: # If code extraction failed, log and create node with empty code
             logger.error("Drafting failed to produce valid code. Creating node with empty code.")
             code = "# Error: Failed to generate code."

        new_node = Node(plan=plan, code=code)
        logger.info(f"Drafted new node {new_node.id}")
        return new_node

    def _improve(self, parent_node: Node) -> Node:
        if self.use_simple_prompts:
             introduction = (
                 "You are an expert Python programmer improving a machine learning solution. "
                 "You are given the previous code and its results. "
                 "Briefly outline a SINGLE specific improvement, then rewrite the FULL Python code implementing ONLY that change."
             )
             improvement_sketch_guideline = [
                 "Focus on ONE small, specific improvement (e.g., change model, add feature engineering step, tune one hyperparameter).",
                 "Explain your proposed improvement in 2-4 sentences.",
                 "Refer to the 'Previous Solution Code' and 'Task Description'.",
                 "Consider the 'Memory / Previous Attempts' to avoid repeating failed ideas.",
             ]
        else:
            # Original complex prompt
            introduction = (
                "You are a Kaggle grandmaster attending a competition. You are provided with a previously developed "
                "solution below and should improve it in order to further increase the (test time) performance. "
                "For this you should first outline a brief plan in natural language for how the solution can be improved and "
                "then implement this improvement in Python based on the provided previous solution. "
            )
            if self.acfg.obfuscate:
                introduction = (
                    "You are an expert machine learning engineer attempting a task. You are provided with a previously developed "
                    "solution below and should improve it in order to further increase the (test time) performance. "
                    "For this you should first outline a brief plan in natural language for how the solution can be improved and "
                    "then implement this improvement in Python based on the provided previous solution. "
                )
            improvement_sketch_guideline = [
                "The solution sketch should be a brief natural language description of how the previous solution can be improved.",
                "You should be very specific and should only propose a single actionable improvement.",
                "This improvement should be atomic so that we can experimentally evaluate the effect of the proposed change.",
                "Take the Memory section into consideration when proposing the improvement.",
                "The solution sketch should be 3-5 sentences.",
                "Don't suggest to do EDA.",
            ]
        # Simplify Memory for OS models if needed
        memory_summary = self.journal.generate_summary()
        if self.use_simple_prompts:
             best_prev = self.journal.get_best_node() # May not be the parent
             if best_prev and best_prev != parent_node:
                 memory_summary = (
                     f"Parent Attempt (Metric: {parent_node.metric.value}):\nPlan: {parent_node.plan}\nResults: {parent_node.analysis}\n\n"
                     f"Best Overall Attempt (Metric: {best_prev.metric.value}):\nPlan: {best_prev.plan}\nResults: {best_prev.analysis}"
                 )
             else:
                 memory_summary = f"Parent Attempt (Metric: {parent_node.metric.value}):\nPlan: {parent_node.plan}\nResults: {parent_node.analysis}\n"

        prompt: Any = {
            "Introduction": introduction,
            "Task Description": self.task_desc,
            # Use simplified memory for simple prompts
            "Memory / Previous Attempts": memory_summary,
            "Previous Solution Code": wrap_code(parent_node.code), # Always include parent code
            "Instructions": {},
        }
        prompt["Instructions"] |= self._prompt_resp_fmt
        prompt["Instructions"]["Improvement Plan Guideline"] = improvement_sketch_guideline
        prompt["Instructions"] |= self._prompt_impl_guideline
        # Environment less critical for improvement? Optional.
        # prompt["Instructions"] |= self._prompt_environment

        plan, code = self.plan_and_code_query(prompt)
        if not code:
             logger.error(f"Improvement step for node {parent_node.id} failed to produce code.")
             code = "# Error: Failed to generate improved code."
        new_node = Node(plan=plan, code=code, parent=parent_node)
        logger.info(f"Improved node {parent_node.id} to create new node {new_node.id}")
        return new_node

    def _debug(self, parent_node: Node) -> Node:
        if self.use_simple_prompts:
             introduction = (
                 "You are an expert Python programmer debugging code. "
                 "The previous code failed or produced an error, shown in the 'Execution Output'. "
                 "Briefly explain the likely cause and the fix in your plan, then provide the corrected FULL Python code."
             )
             bugfix_sketch_guideline = [
                 "Identify the error from the 'Execution Output'.",
                 "Explain the likely cause and your fix in 2-4 sentences.",
                 "Refer to the 'Buggy Code'.",
             ]
        else:
            # Original complex prompt
            introduction = (
                "You are a Kaggle grandmaster attending a competition. "
                "Your previous solution had a bug and/or did not produce a submission.csv, "
                "so based on the information below, you should revise it in order to fix this. "
                "Your response should be an implementation outline in natural language,"
                " followed by a single markdown code block which implements the bugfix/solution."
            )
            if self.acfg.obfuscate:
                 introduction = (
                     "You are an expert machine learning engineer attempting a task. "
                     "Your previous solution had a bug and/or did not produce a submission.csv, "
                     "so based on the information below, you should revise it in order to fix this. "
                     "Your response should be an implementation outline in natural language,"
                     " followed by a single markdown code block which implements the bugfix/solution."
                 )
            bugfix_sketch_guideline = [
                "You should write a brief natural language description (3-5 sentences) of how the issue in the previous implementation can be fixed.",
                "Don't suggest to do EDA.",
            ]

        prompt: Any = {
            "Introduction": introduction,
            "Task Description": self.task_desc,
            "Buggy Code": wrap_code(parent_node.code),
            "Execution Output / Error": wrap_code(parent_node.term_out, lang=""), # Use raw term_out for debugging
            "Instructions": {},
        }
        prompt["Instructions"] |= self._prompt_resp_fmt
        prompt["Instructions"]["Bugfix Plan Guideline"] = bugfix_sketch_guideline
        prompt["Instructions"] |= self._prompt_impl_guideline

        if self.acfg.data_preview and self.data_preview:
            prompt["Data Overview"] = self.data_preview

        plan, code = self.plan_and_code_query(prompt)
        if not code:
             logger.error(f"Debugging step for node {parent_node.id} failed to produce code.")
             code = "# Error: Failed to generate debugged code."
        new_node = Node(plan=plan, code=code, parent=parent_node)
        logger.info(f"Debugged node {parent_node.id} to create new node {new_node.id}")
        return new_node

    def reflect(self, code: str) -> tuple[str, str]:
        """
        Performs a two-step self-reflection using the few-shot utility function.
        """
        logger.info("Initiating two-step self-reflection (few-shot)...")
        try:
            # Use the few-shot version
            reflection_plan, revised_code = perform_two_step_reflection_with_fewshot(
                code=code,
                task_desc=self.task_desc,
                model_name=self.acfg.code.model, # Use coding model for reflection edits
                temperature=self.acfg.code.temp, # Use coding temp
                convert_system_to_user=self.acfg.convert_system_to_user,
                query_func=query,
                wrap_code_func=wrap_code,
                extract_code_func=extract_code,
                 # Pass relevant parts of config if needed by backends
                hf_local=self.cfg.get('hf_local'),
                vllm=self.cfg.get('vllm'),
            )

            # Check plan content to log appropriately
            if reflection_plan.strip() == "No specific errors found requiring changes.":
                logger.info("Self-reflection found no specific errors requiring changes.")
            elif revised_code and revised_code != code:
                 logger.info("Self-reflection resulted in code changes.")
                 # logger.debug(f"Reflection Plan:\n{reflection_plan}") # Optionally log plan
            else:
                 # This case means a plan was generated but the code didn't change or extraction failed
                 logger.warning("Self-reflection generated a plan but no code changes were applied (or extraction failed).")
                 # logger.debug(f"Reflection Plan:\n{reflection_plan}") # Log the plan for debugging

            # Return original code if revised_code is empty or same as original
            return reflection_plan, revised_code if revised_code and revised_code != code else code

        except Exception as e:
            logger.error(f"Error during self-reflection: {e}", exc_info=True)
            # Fallback to original code in case of error
            return "Reflection error occurred.", code


    def update_data_preview(
        self,
    ):
        try:
             self.data_preview = data_preview.generate(self.cfg.workspace_dir / "input") # Generate from input subdir
             logger.info("Data preview updated.")
        except Exception as e:
             logger.error(f"Failed to generate data preview: {e}", exc_info=True)
             self.data_preview = "Error: Could not generate data preview."


    def step(self, exec_callback: ExecCallbackType):
        # Clear the submission dir
        submission_dir = self.cfg.workspace_dir / "submission"
        shutil.rmtree(submission_dir, ignore_errors=True)
        submission_dir.mkdir(exist_ok=True)

        if self.data_preview is None:
            self.update_data_preview()

        parent_node = self.search_policy()
        logger.info(f"Agent step {self.current_step+1}/{self.acfg.steps}. Parent node: {parent_node.id if parent_node else 'None'}")

        draft_flag = False
        if parent_node is None:
            draft_flag = True
            result_node = self._draft()
        elif parent_node.is_buggy:
            result_node = self._debug(parent_node)
        else:
            result_node = self._improve(parent_node)

        # Perform self-reflection, especially on drafts or if enabled for all steps
        # For OS models, reflecting on the initial draft is often beneficial.
        # if draft_flag: # Reflect only on drafts
        # Reflect on every generated code attempt before execution
        reflection_plan, reflected_code = self.reflect(code=result_node.code)
        if reflected_code != result_node.code:
            logger.info(f"Node {result_node.id} code updated after self-reflection.")
            # Optionally store the plan: result_node.reflection_plan = reflection_plan
            result_node.code = reflected_code
        else:
             logger.info(f"Node {result_node.id} code unchanged after self-reflection.")


        # Proceed with execution
        logger.info(f"Agent executing code for node {result_node.id}")
        exec_result = exec_callback(result_node.code, True) # Always reset session for simplicity now
        result_node = self.parse_exec_result(node=result_node, exec_result=exec_result)

        # Check if submission.csv was actually created
        submission_file = self.cfg.workspace_dir / "submission" / "submission.csv"
        has_submission = submission_file.exists() and submission_file.stat().st_size > 0

        if not result_node.is_buggy and not has_submission:
            result_node.is_buggy = True
            result_node.metric = WorstMetricValue()
            result_node.analysis = (result_node.analysis or "") + "\nError: submission.csv not found or empty in ./submission/ directory."
            logger.warning(f"Node {result_node.id} marked as buggy: submission.csv missing/empty.")
        elif not result_node.is_buggy and has_submission:
             result_node.analysis = (result_node.analysis or "") + "\nInfo: submission.csv successfully created."
        elif result_node.is_buggy and has_submission:
             # Buggy for other reasons, but submission exists
             result_node.analysis = (result_node.analysis or "") + "\nWarning: submission.csv created, but node marked as buggy due to other errors/metric issues."

        self.journal.append(result_node)

        # Cache best solution
        best_node = self.journal.get_best_node()
        if best_node is not None and best_node.id == result_node.id:
            logger.info(f"Node {result_node.id} is the new best node (Metric: {result_node.metric}). Caching solution.")
            best_solution_dir = self.cfg.workspace_dir / "best_solution"
            best_solution_dir.mkdir(exist_ok=True, parents=True)
            best_submission_dir = self.cfg.workspace_dir / "best_submission" # Keep separate maybe?
            best_submission_dir.mkdir(exist_ok=True, parents=True)

            # Copy submission.csv if it exists
            if submission_file.exists():
                 try:
                      shutil.copy2(submission_file, best_submission_dir / "submission.csv")
                 except Exception as e:
                      logger.error(f"Failed to copy best submission.csv: {e}")

            # Save best solution code and node id
            try:
                with open(best_solution_dir / "solution.py", "w") as f:
                    f.write(result_node.code)
                with open(best_solution_dir / "node_id.txt", "w") as f:
                    f.write(str(result_node.id))
            except Exception as e:
                 logger.error(f"Failed to write best solution files: {e}")
        elif best_node is not None:
            logger.info(f"Node {result_node.id} (Metric: {result_node.metric}) is not better than best node {best_node.id} (Metric: {best_node.metric}).")
        else:
             logger.info(f"Node {result_node.id} processed. No valid best node yet.")

        self.current_step += 1


    def parse_exec_result(self, node: Node, exec_result: ExecutionResult) -> Node:
        """Parses execution result using LLM feedback."""
        logger.info(f"Agent parsing execution results for node {node.id}")

        node.absorb_exec_result(exec_result)

        # Simplify intro for OS models if needed
        if self.use_simple_prompts:
             introduction = (
                 "You are reviewing the output of a Python script for an ML task. "
                 "Determine if it failed (bug), if it created `./submission/submission.csv`, and report the validation metric value."
             )
        else:
            # Original complex prompt
            introduction = (
                "You are a Kaggle grandmaster attending a competition. "
                "You have written code to solve this task and now need to evaluate the output of the code execution. "
                "You should determine if there were any bugs as well as report the empirical findings."
            )
            if self.acfg.obfuscate:
                 introduction = (
                     "You are an expert machine learning engineer attempting a task. "
                     "You have written code to solve this task and now need to evaluate the output of the code execution. "
                     "You should determine if there were any bugs as well as report the empirical findings."
                 )

        prompt = {
            "Introduction": introduction,
            "Task Description": self.task_desc, # Provide task context
            "Code Executed": wrap_code(node.code),
            "Execution Output Log": wrap_code(node.term_out, lang=""), # Use raw term_out
        }

        try:
             # Use feedback model specified in config
             response = cast(
                 dict,
                 query(
                     system_message=prompt,
                     user_message="Evaluate the execution using the 'submit_review' function.", # Explicit instruction
                     func_spec=review_func_spec,
                     model=self.acfg.feedback.model,
                     temperature=self.acfg.feedback.temp,
                     convert_system_to_user=self.acfg.convert_system_to_user,
                     # Pass relevant parts of config if needed by backends
                     hf_local=self.cfg.get('hf_local'),
                     vllm=self.cfg.get('vllm'),
                 ),
             )
        except Exception as e:
             logger.error(f"Error during feedback query for node {node.id}: {e}", exc_info=True)
             # Handle error: Mark as buggy, provide default analysis
             node.analysis = f"Error during feedback analysis: {e}"
             node.is_buggy = True
             node.metric = WorstMetricValue()
             return node

        # Validate response structure (basic check)
        if not all(k in response for k in ["is_bug", "has_csv_submission", "summary", "metric", "lower_is_better"]):
             logger.error(f"Feedback response for node {node.id} has missing keys: {response}")
             node.analysis = "Error: Feedback LLM response format incorrect."
             node.is_buggy = True
             node.metric = WorstMetricValue()
             return node


        # Check if metric is a valid number
        metric_value = response.get("metric")
        is_metric_valid = isinstance(metric_value, (int, float)) and not np.isnan(metric_value) and not np.isinf(metric_value)

        # Determine buggy status
        # Buggy if: LLM says it is, code execution raised an exception, metric is invalid,
        # OR LLM says no submission.csv was produced (we double-check this later in step())
        node.is_buggy = (
            response.get("is_bug", True) # Default to buggy if key missing
            or node.exc_type is not None
            or not is_metric_valid
            or not response.get("has_csv_submission", False) # If LLM claims no submission, mark buggy for now
        )
        node.analysis = response.get("summary", "No analysis provided.")

        if node.is_buggy:
            logger.info(f"Parsed results: Node {node.id} is buggy.")
            if node.exc_type:
                 logger.info(f"Reason: Exception during execution ({node.exc_type}).")
            elif not is_metric_valid:
                 logger.info(f"Reason: Invalid metric value ({metric_value}).")
            elif not response.get("has_csv_submission", False):
                 logger.info("Reason: Feedback LLM reported no submission.csv (will be verified).")
            else:
                 logger.info("Reason: Feedback LLM reported is_bug=true.")
            node.metric = WorstMetricValue()
        else:
            # Ensure lower_is_better is boolean, default to None if missing/invalid
            lower_is_better = response.get("lower_is_better")
            maximize_metric = not lower_is_better if isinstance(lower_is_better, bool) else None
            
            node.metric = MetricValue(
                value=float(metric_value), # Already validated as float/int
                maximize=maximize_metric
            )
            logger.info(f"Parsed results: Node {node.id} is not buggy. Metric: {node.metric}")
            if maximize_metric is None:
                 logger.warning(f"Optimization direction (lower_is_better) for metric on node {node.id} is unknown or invalid.")

        return node

# END OF MODIFIED FILE aide-ds/aide/agent.py
```

**6. Modified Self-Reflection Utility (`self_reflection.py`) with Few-Shot**

```python
# START OF MODIFIED FILE aide-ds/aide/utils/self_reflection.py

from typing import Any, Callable
import re
import logging

logger = logging.getLogger("aide")

# Define necessary type hints for the functions being passed
QueryFuncType = Callable[..., str] # Simplified type hint for query
WrapCodeFuncType = Callable[[str, str], str] # Added lang argument
ExtractCodeFuncType = Callable[[str], str] # Simplified type hint for extract_code

# --- Few-Shot Example Definition ---
# Example Input Code (with a common error)
EXAMPLE_INPUT_CODE = """
import pandas as pd
import numpy as np

# Simulate loading data
try:
    train_df = pd.read_csv('./input/train.csv')
    test_df = pd.read_csv('./input/test.csv')
    sample_submission = pd.read_csv('./input/sample_submission.csv')
except FileNotFoundError:
    # Fallback for testing if files aren't present
    train_df = pd.DataFrame({'feature1': np.random.rand(100), 'target': np.random.randint(0, 2, 100), 'id': range(100)})
    test_df = pd.DataFrame({'feature1': np.random.rand(50), 'id': range(100, 150)})
    sample_submission = pd.DataFrame({'id': range(100, 150), 'target': np.zeros(50)})

# Simulate predictions (replace with actual model logic)
# Ensuring predictions match the length of the test set
predictions = np.random.rand(len(test_df))

# Create submission DataFrame using test_df's id column
submission_df = pd.DataFrame({'id': test_df['id'], 'target': predictions})

# Save submission - INCORRECT PATH & FILENAME
submission_df.to_csv('my_submission.csv', index=False) # <<< ERROR HERE

print("Submission file created.")
print("Validation Metric: 0.85") # Example metric print
"""

# Expected Output for Stage 1 (Critique) based on EXAMPLE_INPUT_CODE
EXAMPLE_STAGE1_OUTPUT_CRITIQUE = """The main mistake is saving the submission file with an incorrect name and in the wrong directory, it must be './submission/submission.csv'.
1. Line 22: Change the filename from `'my_submission.csv'` to `'submission.csv'`.
2. Line 22: Change the file path to include the target directory, making it `'./submission/submission.csv'`.
"""

# Expected Output for Stage 2 (Code Edit) based on EXAMPLE_INPUT_CODE and EXAMPLE_STAGE1_OUTPUT_CRITIQUE
EXAMPLE_STAGE2_OUTPUT_CODE = """# Applying edits based on review.
```python
import pandas as pd
import numpy as np

# Simulate loading data
try:
    train_df = pd.read_csv('./input/train.csv')
    test_df = pd.read_csv('./input/test.csv')
    sample_submission = pd.read_csv('./input/sample_submission.csv')
except FileNotFoundError:
    # Fallback for testing if files aren't present
    train_df = pd.DataFrame({'feature1': np.random.rand(100), 'target': np.random.randint(0, 2, 100), 'id': range(100)})
    test_df = pd.DataFrame({'feature1': np.random.rand(50), 'id': range(100, 150)})
    sample_submission = pd.DataFrame({'id': range(100, 150), 'target': np.zeros(50)})

# Simulate predictions (replace with actual model logic)
# Ensuring predictions match the length of the test set
predictions = np.random.rand(len(test_df))

# Create submission DataFrame using test_df's id column
submission_df = pd.DataFrame({'id': test_df['id'], 'target': predictions})

# Save submission - CORRECTED PATH & FILENAME
submission_df.to_csv('./submission/submission.csv', index=False) # <<< FIXED HERE

print("Submission file created.")
print("Validation Metric: 0.85") # Example metric print

```"""
# --- End Few-Shot Example Definition ---

def perform_two_step_reflection_with_fewshot(
    code: str,
    task_desc: str,
    model_name: str,
    temperature: float,
    convert_system_to_user: bool,
    query_func: QueryFuncType,
    wrap_code_func: WrapCodeFuncType,
    extract_code_func: ExtractCodeFuncType,
    **kwargs # Capture other args like hf_local, vllm
) -> tuple[str, str]:
    """
    Performs a two-step self-reflection with a few-shot example included in prompts.

    1. Critiques the code and proposes minimal text-based edits (guided by an example).
    2. Applies only those edits to the original code (guided by an example).

    Args:
        code: The code string to reflect upon.
        task_desc: The description of the task for context.
        model_name: Name of the language model to use.
        temperature: Temperature setting for the language model.
        convert_system_to_user: Flag for handling system messages.
        query_func: The function used to query the language model.
        wrap_code_func: Function to wrap code for prompts.
        extract_code_func: Function to extract code from LLM responses.
        **kwargs: Additional arguments to pass to the query function.

    Returns:
        Tuple: (reflection_plan, revised_code)
               - reflection_plan: Text describing the critique and planned edits.
               - revised_code: The minimally revised code, or original if no changes/errors.
    """
    # --- Stage 1: Critique and Edit Proposal (with Few-Shot Example) ---
    critique_prompt = {
        "Role": "You are a meticulous Python code reviewer focused on finding small, critical errors.",
        "Task": (
            "1. Carefully review the 'Code to Review' section below.\n"
            "2. Identify 1 to 4 specific, small mistakes. Focus on: typos, incorrect variable names, simple logic errors (e.g., off-by-one), incorrect file paths (especially the required `./submission/submission.csv`), or missing imports.\n"
            "3. Write a concise summary sentence explaining the main error(s).\n"
            "4. Provide step-by-step, text-only instructions (numbered list) to fix ONLY those specific mistakes.\n"
            "5. If you find NO specific errors that need changing, respond ONLY with the exact sentence: No specific errors found requiring changes.\n"
            "6. STRICTLY follow the format shown in the 'EXAMPLE'."
        ),
        "EXAMPLE": (
            "### EXAMPLE START ###\n"
            "Input Code:\n"
            f"{wrap_code_func(EXAMPLE_INPUT_CODE, lang='python')}\n\n" # Use wrap_code_func
            "Expected Output:\n"
            f"{EXAMPLE_STAGE1_OUTPUT_CRITIQUE}\n"
            "### EXAMPLE END ###"
        ),
        "Rules": (
            "RULE 1: **ABSOLUTELY NO PYTHON CODE in your response.** Only text instructions.\n"
            "RULE 2: Only fix small, clear mistakes. Do NOT suggest improvements, refactoring, or new features.\n"
            "RULE 3: Ensure file paths match requirements (e.g., `./submission/submission.csv`).\n"
            "RULE 4: Follow the 'Output Format' EXACTLY, like the example."
        ),
        "Output Format": (
            "If mistakes are found:\n"
            "- Start with one sentence explaining the main mistake(s).\n"
            "- Then, write a NUMBERED list of fix instructions.\n"
            "- Each number is ONE simple step (e.g., '1. Line 17: Change `my_submission.csv` to `./submission/submission.csv`.').\n"
            "\n"
            "If no mistakes are found:\n"
            "- Write ONLY this sentence: No specific errors found requiring changes."
        ),
        "Code to Review": wrap_code_func(code, lang='python'), # Use the passed function
        "Context": task_desc,
    }

    logger.debug(f"Critique Prompt (Stage 1):\n{critique_prompt}")

    plan_raw = query_func(
        system_message=critique_prompt,
        user_message=None, # User message can be empty if prompt is detailed
        model=model_name,
        temperature=temperature, # Consider lower temp (e.g., 0.1) for critique
        convert_system_to_user=convert_system_to_user,
        **kwargs # Pass extra args
    )
    reflection_plan = re.sub(r"<think>.*?</think>", "", plan_raw, flags=re.DOTALL).strip()
    logger.debug(f"Critique Response (Stage 1 Raw):\n{plan_raw}")
    logger.info(f"Critique Plan (Stage 1 Cleaned):\n{reflection_plan}")


    if not reflection_plan or reflection_plan.strip() == "No specific errors found requiring changes.":
        logger.info("Reflection Step 1: No changes suggested.")
        return reflection_plan or "No specific errors found requiring changes.", code

    # --- Stage 2: Focused Code Edit (with Few-Shot Example) ---
    coder_prompt = {
        "Role": "You are a precise code editor. You only apply the given text instructions.",
        "Task": (
            "1. Take the 'Original Code'.\n"
            "2. Apply *ONLY* the changes described in the 'Edit Instructions' (which are text, not code).\n"
            "3. Output the *entire, modified* code within a single Python code block.\n"
            "4. Follow the 'Output Format' EXACTLY."
        ),
        "EXAMPLE": (
            "### EXAMPLE START ###\n"
            "Original Code:\n"
            f"{wrap_code_func(EXAMPLE_INPUT_CODE, lang='python')}\n\n"
            "Edit Instructions:\n"
            f"{EXAMPLE_STAGE1_OUTPUT_CRITIQUE}\n\n"
            "Expected Output:\n"
            f"{EXAMPLE_STAGE2_OUTPUT_CODE}\n"
            "### EXAMPLE END ###"
        ),
        "Rules": (
            "RULE 1: Apply ONLY the numbered steps from 'Edit Instructions'.\n"
            "RULE 2: **DO NOT change any other part of the code.** No reformatting, no adding comments (except the first required one), no restructuring.\n"
            "RULE 3: Ignore any Python code examples *within* the 'Edit Instructions'; only follow the text steps.\n"
            "RULE 4: Your entire output MUST follow the 'Output Format'."
        ),
        "Output Format": (
            "# Applying edits based on review.\n" # MUST be the very first line
            "```python\n"
            "[The FULL original code, with ONLY the requested edits applied]\n"
            "```\n"
            "**IMPORTANT: NO TEXT before the first '#' comment. NO TEXT after the final '```'.**"
        ),
        "Original Code": wrap_code_func(code, lang='python'),
        "Edit Instructions": reflection_plan,
    }

    logger.debug(f"Coder Prompt (Stage 2):\n{coder_prompt}")

    revised_code_response = query_func(
        system_message=coder_prompt,
        user_message=None,
        model=model_name,
        temperature=0.0, # Use temp 0 for deterministic editing
        convert_system_to_user=convert_system_to_user,
        **kwargs # Pass extra args
    )

    logger.debug(f"Coder Response (Stage 2 Raw):\n{revised_code_response}")
    revised_code = extract_code_func(revised_code_response)

    # Post-processing: Remove the mandatory comment if it exists
    if revised_code and revised_code.startswith("# Applying edits based on review.\n"):
        revised_code = revised_code.split('\n', 1)[1]
        # Also remove potential leading/trailing whitespace again after splitting
        revised_code = revised_code.strip() if revised_code else ""


    if not revised_code:
        logger.warning("Reflection Step 2: Code extraction failed. Returning original code.")
        return reflection_plan, code
    else:
        logger.info("Reflection Step 2: Successfully generated revised code.")
        return reflection_plan, revised_code

# END OF MODIFIED FILE aide-ds/aide/utils/self_reflection.py
```

**7. New Finetuning Scripts**

```python
# START OF NEW FILE aide-ds/finetuning/requirements-finetune.txt
transformers>=4.38.0,<4.42.0 # Pin versions for stability
trl>=0.8.0,<0.9.0
peft>=0.10.0,<0.12.0
accelerate>=0.28.0
bitsandbytes>=0.41.0
datasets>=2.10.0
torch>=2.0.0
# Add other potential dependencies like sentencepiece if needed by tokenizer
# END OF NEW FILE aide-ds/finetuning/requirements-finetune.txt
```

```python
# START OF NEW FILE aide-ds/finetuning/prepare_data.py
import json
import argparse
from pathlib import Path
import logging
import sys
from typing import List, Dict, Optional

# Add the parent directory of 'aide' to sys.path to allow importing aide modules
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent
sys.path.insert(0, str(project_root.parent))

try:
    from aide.journal import Journal, Node
    from aide.utils.serialize import load_json
except ImportError as e:
    print(f"Error importing AIDE modules: {e}")
    print("Please ensure the script is run from within the project structure or the 'aide-ds' directory is in the Python path.")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def format_alpaca(instruction: str, input_str: Optional[str], output: str) -> Dict:
    """Formats data into Alpaca instruction-following format."""
    if input_str:
        text = f"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input_str}\n\n### Response:\n{output}"
    else:
        text = f"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n{output}"
    # You might want separate fields depending on the trainer library (e.g., SFTTrainer often takes 'text' or instruction/output directly)
    return {"text": text, "instruction": instruction, "input": input_str or "", "output": output}

def create_instruction_pairs(journal: Journal, task_desc: str) -> List[Dict]:
    """
    Extracts instruction-following pairs from the AIDE journal.
    Focuses on code generation steps (draft, improve, debug).
    """
    pairs = []
    nodes_by_id = {node.id: node for node in journal.nodes}

    for node in journal.nodes:
        # Skip nodes without code or plan (shouldn't happen often)
        if not node.code or not node.plan:
            continue

        instruction = ""
        input_str = ""
        output = node.code # The generated code is always the target output

        # --- Handle Draft Nodes ---
        if node.parent is None:
            instruction = f"Given the following machine learning task description, first write a brief plan and then the Python code to implement it.\n\nTask Description:\n{task_desc}"
            input_str = f"Plan:\n{node.plan}" # Include the agent's plan as input context
            if node.plan and node.code: # Only add if both plan and code exist
                 pairs.append(format_alpaca(instruction, input_str, output))

        # --- Handle Improve/Debug Nodes ---
        elif node.parent:
            parent_node = nodes_by_id.get(node.parent.id)
            if not parent_node:
                logger.warning(f"Parent node {node.parent.id} not found for node {node.id}. Skipping.")
                continue

            if parent_node.is_buggy: # Debugging Step
                instruction = f"The following Python code for a machine learning task has a bug, resulting in the error output shown. Briefly explain the fix and provide the corrected full Python code.\n\nTask Description:\n{task_desc}"
                input_str = (f"Buggy Code:\n```python\n{parent_node.code}\n```\n\n"
                             f"Execution Output/Error:\n```\n{parent_node.term_out}\n```\n\n" # Use raw term_out
                             f"Plan for Fix:\n{node.plan}")
                if node.plan and node.code:
                     pairs.append(format_alpaca(instruction, input_str, output))

            else: # Improvement Step
                instruction = f"Improve the following Python code for a machine learning task based on the provided plan. Output the complete, improved Python code.\n\nTask Description:\n{task_desc}"
                input_str = (f"Previous Code:\n```python\n{parent_node.code}\n```\n\n"
                             f"Improvement Plan:\n{node.plan}")
                if node.plan and node.code:
                     pairs.append(format_alpaca(instruction, input_str, output))

        # Could potentially add pairs for self-reflection critique/edit steps here
        # if node.reflection_plan and node.original_code: ...

    logger.info(f"Extracted {len(pairs)} instruction pairs from journal.")
    return pairs

def main():
    parser = argparse.ArgumentParser(description="Prepare AIDE journal data for fine-tuning.")
    parser.add_argument("log_dir", type=str, help="Directory containing AIDE experiment log folders (each with a journal.json).")
    parser.add_argument("output_file", type=str, help="Path to save the output JSONL file.")
    parser.add_argument("--task_desc_file", type=str, default=None, help="Optional: Path to a single task description file to use for all journals.")

    args = parser.parse_args()

    log_dir = Path(args.log_dir)
    output_file = Path(args.output_file)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    global_task_desc = None
    if args.task_desc_file:
        try:
            with open(args.task_desc_file, 'r') as f:
                global_task_desc = f.read()
            logger.info(f"Using global task description from: {args.task_desc_file}")
        except FileNotFoundError:
            logger.error(f"Global task description file not found: {args.task_desc_file}")
            sys.exit(1)

    total_pairs = 0
    with open(output_file, 'w') as outfile:
        for exp_dir in log_dir.iterdir():
            if not exp_dir.is_dir():
                continue

            journal_path = exp_dir / "journal.json"
            config_path = exp_dir / "config.yaml" # To get task desc if not global

            if journal_path.exists():
                logger.info(f"Processing journal: {journal_path}")
                try:
                    journal: Journal = load_json(journal_path, Journal)

                    task_desc = global_task_desc
                    if not task_desc:
                        # Try loading task desc from individual experiment config
                        if config_path.exists():
                            from omegaconf import OmegaConf # Lazy import
                            cfg = OmegaConf.load(config_path)
                            if cfg.get('desc_file') and Path(cfg.desc_file).exists():
                                with open(cfg.desc_file, 'r') as f:
                                    task_desc = f.read()
                            elif cfg.get('goal'):
                                task_desc = f"Goal: {cfg.goal}"
                                if cfg.get('eval'):
                                    task_desc += f"\nEvaluation: {cfg.eval}"
                            else:
                                logger.warning(f"Could not determine task description for {exp_dir}. Skipping.")
                                continue
                        else:
                            logger.warning(f"Config file not found for {exp_dir}, cannot get task description. Skipping.")
                            continue

                    instruction_pairs = create_instruction_pairs(journal, task_desc)
                    for pair in instruction_pairs:
                        outfile.write(json.dumps(pair) + '\n')
                    total_pairs += len(instruction_pairs)
                except Exception as e:
                    logger.error(f"Failed to process journal {journal_path}: {e}", exc_info=True)
            else:
                logger.warning(f"Journal file not found in {exp_dir}")

    logger.info(f"Finished processing. Total instruction pairs written: {total_pairs}")
    logger.info(f"Output saved to: {output_file}")

if __name__ == "__main__":
    main()
# END OF NEW FILE aide-ds/finetuning/prepare_data.py
```

```python
# START OF NEW FILE aide-ds/finetuning/finetune_deepseek.py
import argparse
import logging
import os
from pathlib import Path

import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from trl import SFTTrainer

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    parser = argparse.ArgumentParser(description="Fine-tune DeepSeek-7B model using QLoRA.")
    parser.add_argument("--model_id", type=str, default="deepseek-ai/deepseek-coder-7b-instruct-v1.5", help="Hugging Face model ID.")
    parser.add_argument("--dataset_path", type=str, required=True, help="Path to the prepared JSONL dataset file.")
    parser.add_argument("--output_dir", type=str, default="./deepseek-7b-finetuned-adapters", help="Directory to save the fine-tuned adapters.")

    # --- Training Arguments ---
    parser.add_argument("--batch_size", type=int, default=4, help="Per device batch size.")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4, help="Gradient accumulation steps.")
    parser.add_argument("--learning_rate", type=float, default=2e-4, help="Learning rate.")
    parser.add_argument("--num_train_epochs", type=int, default=1, help="Number of training epochs.")
    parser.add_argument("--max_seq_length", type=int, default=2048, help="Maximum sequence length.")
    parser.add_argument("--logging_steps", type=int, default=10, help="Log training status every N steps.")
    parser.add_argument("--save_steps", type=int, default=100, help="Save checkpoint every N steps.")
    parser.add_argument("--warmup_ratio", type=float, default=0.03, help="Warmup ratio for learning rate scheduler.")
    parser.add_argument("--lr_scheduler_type", type=str, default="cosine", help="Learning rate scheduler type.")
    parser.add_argument("--optim", type=str, default="paged_adamw_32bit", help="Optimizer type.")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="Enable gradient checkpointing.")
    parser.add_argument("--bf16", action="store_true", help="Use bfloat16 precision (if available).")
    parser.add_argument("--report_to", type=str, default="none", help="Where to report metrics (e.g., 'wandb', 'tensorboard', 'none').")

    # --- LoRA Arguments ---
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank (r).")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha.")
    parser.add_argument("--lora_dropout", type=float, default=0.05, help="LoRA dropout.")
    # Common target modules for DeepSeek Coder v1.5 (verify if needed)
    parser.add_argument("--target_modules", nargs='+', default=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], help="Modules to apply LoRA to.")

    args = parser.parse_args()

    logger.info(f"Starting fine-tuning with arguments: {args}")

    # 1. Load Dataset
    logger.info(f"Loading dataset from {args.dataset_path}")
    try:
        # Assuming the dataset was prepared with a 'text' field containing the formatted prompt
        dataset = load_dataset("json", data_files=args.dataset_path, split="train")
        # Optional: split into train/eval
        # dataset = dataset.train_test_split(test_size=0.05)
        # train_dataset = dataset["train"]
        # eval_dataset = dataset["test"]
        logger.info(f"Dataset loaded: {dataset}")
    except Exception as e:
        logger.error(f"Failed to load dataset: {e}", exc_info=True)
        return

    # 2. Load Tokenizer
    logger.info(f"Loading tokenizer for {args.model_id}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token # Set pad token if missing
        logger.info(f"Set tokenizer pad_token to eos_token: {tokenizer.eos_token}")
    tokenizer.padding_side = "right" # Default, important for some models

    # 3. Configure Quantization (QLoRA)
    # Check CUDA availability for BitsAndBytes
    use_4bit = torch.cuda.is_available()
    if use_4bit:
        compute_dtype = torch.bfloat16 if args.bf16 else torch.float16
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=compute_dtype,
            bnb_4bit_use_double_quant=True, # Nested quantization
        )
        logger.info(f"Using 4-bit QLoRA with compute_dtype={compute_dtype}")
    else:
        bnb_config = None
        logger.warning("CUDA not available. Loading model in full precision (no QLoRA).")

    # 4. Load Base Model
    logger.info(f"Loading base model: {args.model_id}")
    model = AutoModelForCausalLM.from_pretrained(
        args.model_id,
        quantization_config=bnb_config if use_4bit else None,
        device_map="auto", # Automatically distribute across GPUs if available
        trust_remote_code=True,
        torch_dtype=compute_dtype if use_4bit else None # Set dtype if quantized
    )
    model.config.use_cache = False # Disable cache for training stability with gradient checkpointing
    if args.gradient_checkpointing:
        model.config.pretraining_tp = 1 # May be needed for gradient checkpointing compatibility

    # 5. Configure LoRA
    logger.info(f"Configuring LoRA with r={args.lora_r}, alpha={args.lora_alpha}, modules={args.target_modules}")
    peft_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        target_modules=args.target_modules,
        bias="none",
        task_type="CAUSAL_LM",
    )

    # Prepare model for k-bit training if using quantization
    if use_4bit:
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)

    # Apply PEFT config - this modifies the model in place
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # 6. Configure Training Arguments
    training_arguments = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        optim=args.optim,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        learning_rate=args.learning_rate,
        weight_decay=0.001,
        fp16=not args.bf16 and use_4bit, # Use fp16 if not bf16 and quantized
        bf16=args.bf16 and use_4bit, # Use bf16 if specified and quantized/GPU supports
        max_grad_norm=0.3,
        max_steps=-1, # Override with num_train_epochs
        warmup_ratio=args.warmup_ratio,
        group_by_length=True, # Group sequences of similar length for efficiency
        lr_scheduler_type=args.lr_scheduler_type,
        gradient_checkpointing=args.gradient_checkpointing,
        report_to=args.report_to,
        save_strategy="steps", # Save checkpoints based on steps
        save_total_limit=3, # Keep only the last few checkpoints
        # evaluation_strategy="steps", # If using eval dataset
        # eval_steps=args.save_steps, # Evaluate at same frequency as saving
    )

    # 7. Initialize Trainer
    # SFTTrainer handles packing sequences by default if max_seq_length is set
    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset, # Use the full loaded dataset for training for now
        # eval_dataset=eval_dataset, # Uncomment if using eval split
        peft_config=peft_config,
        dataset_text_field="text", # Field containing the formatted prompt+completion
        max_seq_length=args.max_seq_length,
        tokenizer=tokenizer,
        args=training_arguments,
        packing=False, # Set to True if dataset is large and sequences are short
                       # Set to False if using full prompts from prepare_data.py as 'text'
        # data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False), # Use if packing=True
    )

    # 8. Train
    logger.info("Starting training...")
    trainer.train()

    # 9. Save Final Adapters
    logger.info(f"Training complete. Saving final adapters to {args.output_dir}")
    trainer.save_model(args.output_dir) # Saves only the LoRA adapters

    # Optional: Save tokenizer too
    tokenizer.save_pretrained(args.output_dir)

    logger.info("Finetuning finished successfully.")

if __name__ == "__main__":
    main()
# END OF NEW FILE aide-ds/finetuning/finetune_deepseek.py
```

```markdown
# START OF NEW FILE aide-ds/finetuning/README.md
# AIDE Finetuning Pipeline

This directory contains scripts to prepare data generated by AIDE runs and fine-tune language models (specifically DeepSeek-7B using QLoRA in this example) on that data.

## 1. Prepare Data (`prepare_data.py`)

This script converts the `journal.json` files from multiple AIDE experiments into a JSONL format suitable for instruction fine-tuning.

**Usage:**

```bash
python prepare_data.py <path_to_aide_log_directory> <output_jsonl_file> [--task_desc_file <path_to_task_desc>]
```

-   `<path_to_aide_log_directory>`: The directory containing subfolders for each AIDE experiment (e.g., `aide-ds/logs/`). Each subfolder should contain a `journal.json` file.
-   `<output_jsonl_file>`: The path where the formatted JSONL data will be saved (e.g., `finetuning_data.jsonl`).
-   `--task_desc_file` (Optional): If all journals correspond to the same task, provide the path to the task description file (e.g., `aide-ds/example_tasks/house_prices.md`). If omitted, the script will try to find the description from each experiment's `config.yaml`.

**Output Format:**

The script generates a JSONL file where each line is a JSON object. By default, it uses a format compatible with `trl.SFTTrainer`'s `dataset_text_field="text"` which combines instruction, input, and output into a single string field, resembling the Alpaca format. It also includes separate fields for potential use with other trainers:

```json
{"text": "Below is an instruction... ### Instruction:\n...\n\n### Input:\n...\n\n### Response:\n...", "instruction": "...", "input": "...", "output": "..."}
{"text": "...", "instruction": "...", "input": "...", "output": "..."}
```

The instructions are derived from the different steps in the AIDE journal (drafting, improving, debugging).

## 2. Fine-tune Model (`finetune_deepseek.py`)

This script fine-tunes a DeepSeek-7B model (or compatible Hugging Face model) using QLoRA on the dataset prepared in Step 1.

**Prerequisites:**

Install the necessary dependencies:

```bash
pip install -r requirements-finetune.txt
```

You also need `torch` installed with CUDA support if you want to use QLoRA (4-bit quantization).

**Usage:**

```bash
# Example using accelerate for multi-GPU or efficient single-GPU training
accelerate launch finetune_deepseek.py \
    --model_id "deepseek-ai/deepseek-coder-7b-instruct-v1.5" \
    --dataset_path "finetuning_data.jsonl" \
    --output_dir "./deepseek-7b-aide-adapters" \
    --batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 2e-4 \
    --num_train_epochs 1 \
    --max_seq_length 2048 \
    --logging_steps 10 \
    --save_steps 100 \
    --bf16 # Use bf16 if your GPU supports it, otherwise remove for fp16

# Add --gradient_checkpointing for very long sequences or larger models to save memory
```

**Key Arguments:**

-   `--model_id`: The Hugging Face identifier for the base model (e.g., `deepseek-ai/deepseek-coder-7b-instruct-v1.5`).
-   `--dataset_path`: Path to the JSONL file created by `prepare_data.py`.
-   `--output_dir`: Where the trained LoRA adapters will be saved.
-   Training parameters (`--batch_size`, `--learning_rate`, `--num_train_epochs`, etc.): Adjust based on your hardware and dataset size.
-   LoRA parameters (`--lora_r`, `--lora_alpha`, etc.): Control the LoRA adaptation.
-   `--bf16`: Enable bfloat16 training (requires Ampere+ GPU). If omitted, uses fp16. QLoRA requires either bf16 or fp16.

**Output:**

The script will save the trained LoRA adapter weights (and tokenizer configuration) to the specified `--output_dir`. These adapters can then be loaded on top of the base model for inference.

# END OF NEW FILE aide-ds/finetuning/README.md
```

**Summary of Changes & Next Steps:**

1.  **Backends:**
    *   New `backend_vllm.py` added.
    *   `backend/__init__.py` updated to dispatch to vLLM.
    *   `backend_local.py` enhanced with chat template support (needs config flag `hf_local.use_chat_template`).
2.  **Configuration:**
    *   `config.yaml` updated with sections for `vllm`, `hf_local`, and `agent.prompt_style`.
    *   Default models changed to reflect potential OS usage.
3.  **Prompts & Reflection:**
    *   `agent.py` modified with conditional logic for `prompt_style="simple"`. Prompts are simplified.
    *   `agent.py` now uses the few-shot reflection function by default.
    *   `self_reflection.py` now includes `perform_two_step_reflection_with_fewshot` and the required example data.
4.  **Finetuning:**
    *   New `finetuning/` directory created.
    *   `requirements-finetune.txt` added.
    *   `prepare_data.py` script created to process journals.
    *   `finetune_deepseek.py` script created for QLoRA fine-tuning.
    *   `finetuning/README.md` added.

**Important User Actions:**

*   **Review Dependencies:** Check `requirements.txt` and `finetuning/requirements-finetune.txt` and install any missing packages (`pip install -r requirements.txt`, `pip install -r finetuning/requirements-finetune.txt`). You might need to install `torch` with CUDA support separately depending on your system.
*   **Configure Backends:**
    *   If using **vLLM**, ensure your vLLM server is running and update `vllm.base_url` in `config.yaml` if it's not on `http://localhost:8000/v1`. Specify the model using the `vllm/` prefix, e.g., `agent.code.model=vllm/deepseek-ai/deepseek-coder-7b-instruct-v1.5`.
    *   If using **Hugging Face local**, ensure you have the necessary VRAM. Specify the model using its Hugging Face path, e.g., `agent.code.model=deepseek-ai/deepseek-coder-7b-instruct-v1.5`. Adjust `hf_local` settings in `config.yaml` if needed.
*   **Configure Prompts:** Decide if you want `"simple"` or `"default"` prompts in `agent.prompt_style` in `config.yaml`. `"simple"` is recommended for smaller open-source models.
*   **Run Finetuning:**
    1.  Run `prepare_data.py` pointing to your AIDE logs directory.
    2.  Run `finetune_deepseek.py` pointing to the prepared dataset and configure training arguments.
*   **Test:** Run AIDE with your chosen backend and prompt style on a sample task.