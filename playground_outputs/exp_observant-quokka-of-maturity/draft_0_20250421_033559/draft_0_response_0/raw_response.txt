Okay, so I need to solve this Kaggle competition problem where I have to predict the author of a sentence as one of three: Poe, Lovecraft, or Shelley. The task is to build a machine learning model that can accurately classify the author of a given text. 

First, I should understand the problem thoroughly. The goal is to predict the author based on the text content. The data provided includes three columns: EAP, HPL, and MWS, which are float values representing different metrics. These metrics likely correspond to the author's style or characteristics. The task is to use this data to predict the author.

I need to plan the approach. Since the task is classification with three possible outcomes, I should consider using a classification model. Multi-class classification is suitable here because there are three distinct classes: Poe, Lovecraft, and Shelley. 

I should start by reading the data to understand the structure. The input has three CSV files: train, test, and sample_submission. The text is in the 'text' column, and the metrics are in 'EAP', 'HPL', and 'MWS'. The task is to predict the author, so the target variable is the author.

Next, I need to decide on the evaluation metric. Logarithmic loss is suggested, which is suitable for multi-class classification as it penalizes predictions based on the confidence of the prediction. This will help in assessing how well the model is performing.

For the model, I can use LightGBM since it's a powerful and efficient algorithm for both classification and regression tasks. It can handle feature engineering and is known for its performance on tabular data.

Now, the plan is to:

1. Read the data from the input directory.
2. Perform feature engineering, such as extracting features from the text.
3. Split the data into training and validation sets to evaluate the model's performance.
4. Train the LightGBM model.
5. Evaluate the model using the validation set.
6. Generate predictions for the test set and save them.

I should also consider data preprocessing steps like tokenization or text cleaning, but since the data is already prepared, I might skip that unless necessary.

In terms of code structure, I'll start by importing necessary libraries. I'll use pandas for data reading, numpy for numerical operations, and xgboost for the model. For feature engineering, I'll use text processing libraries like NLTK or SpaCy, but since the data is already in a CSV format, I might not need to process the text further unless it's too large.

I'll split the data into training and validation sets using a ratio, say 80-20. Then, I'll train the model on the training data and evaluate it on the validation data. Finally, I'll generate predictions for the test data and save them to the specified path.

I need to ensure that the code is clean and well-structured. I'll write the code in a single Python file, as per the instructions. I'll include all necessary imports, read the data, perform feature engineering if needed, split the data, train the model, evaluate, and save the predictions.

I should also make sure to handle any potential issues, like missing values or class imbalance, but since the data is already provided, I'll proceed without additional data preprocessing unless specified.

In summary, the plan is to use LightGBM for multi-class classification, perform feature engineering if needed, split the data, train and evaluate the model, and generate predictions for the test set.
</think>

```python
# Plan
1. Read the input data and prepare the target variable (author) and features (text, EAP, HPL, MWS).
2. Split the data into training and validation sets.
3. Perform feature engineering on the text data.
4. Train a LightGBM model for multi-class classification.
5. Evaluate the model using multi-class logarithmic loss.
6. Generate predictions for the test data and save them.

# Code
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, log_loss
from lightgbm import LGBMClassifier

# Read data
train = pd.read_csv('input/train.csv')
test = pd.read_csv('input/test.csv')
sample_submission = pd.read_csv('input/sample_submission.csv')

# Prepare target and features
X = train[['text', 'EAP', 'HPL', 'MWS']]
y = train['author']

# Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.5, random_state=42)

# Initialize model
lgbm = LGBMClassifier(n_estimators=100, max_depth=10, learning_rate=0.1, num_leaves=100)

# Train model
lgbm.fit(X_train, y_train)

# Evaluate model
y_pred = lgbm.predict(X_val)
loss = log_loss(y_val, y_pred)
print(f"Logarithmic Loss: {loss}")

# Generate predictions for test data
y_test_pred = lgbm.predict(X_test)
np.savetxt('submission/submission.csv', y_test_pred, delimiter=',')