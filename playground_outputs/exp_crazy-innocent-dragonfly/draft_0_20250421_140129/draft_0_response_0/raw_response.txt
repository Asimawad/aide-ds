Okay, so I need to solve this Kaggle competition problem where I have to predict the author of a sentence as one of three: Poe, Lovecraft, or Shelley. The task is to build a machine learning model that can accurately classify the author of a given text. 

First, I should understand the problem thoroughly. The goal is to predict the author based on the text content. The data provided includes three columns: EAP, HPL, and MWS, which are float values representing different metrics. These metrics likely correspond to the author's style or characteristics. The task is to use this data to predict the author.

I need to plan the approach. Since the task is classification with three possible outcomes, I can use a classification model. The evaluation metric should be multi-class logarithmic loss, which is suitable for this scenario. 

For the model, I can use LightGBM since it's a powerful and efficient algorithm for such tasks. It can handle feature engineering and is good for handling large datasets. I'll also need to preprocess the text data, perhaps using TF-IDF or word embeddings to convert the text into a numerical format that the model can learn from.

The plan is to:

1. Read the data and perform basic preprocessing, like splitting the text into training, validation, and test sets.
2. Extract the EAP, HPL, and MWS features from the text.
3. Use LightGBM to train a model that predicts the author based on these features.
4. Evaluate the model using multi-class logarithmic loss on the validation set.
5. Generate predictions for the test set and save them in the required format.

I should also consider data augmentation and cross-validation to ensure the model's robustness. Since the data is already prepared, I don't need to do EDA, but I should ensure that the text features are properly engineered.

Now, for the code. I'll start by importing necessary libraries like pandas for data handling, numpy for numerical operations, and xgboost for the model. I'll also import LightGBM from the lightgbm package. 

Next, I'll read the input data and split it into training, validation, and test sets. I'll extract the text, author, EAP, HPL, and MWS columns. Then, I'll extract the features (EAP, HPL, MWS) and the target (author).

I'll then create a LightGBM classifier and fit it on the training data. After fitting, I'll make predictions on the validation set and compute the multi-class logarithmic loss to evaluate the model's performance.

Finally, I'll generate the predictions for the test set, save them to a CSV file, and print the evaluation metric.

I need to make sure the code is well-structured and follows best practices, such as using clear variable names and proper error handling. Also, since the code is a single file, I'll include all necessary imports at the top and ensure that the script runs without errors.

I should also consider the computational efficiency, especially since the dataset is large. Using efficient feature engineering and proper data splitting will help in handling the data without running into memory or time issues.

In summary, the approach is to build a LightGBM model using the given features to predict the author, evaluate it using the specified metric, and generate predictions for the test data.
</think>

```python
# Plan
1. Read and preprocess the input data
2. Extract features (EAP, HPL, MWS) and target (author)
3. Split data into training, validation, and test sets
4. Use LightGBM for classification
5. Evaluate using multi-class logarithmic loss
6. Generate predictions for test set
7. Save predictions to submission file

# Code
```python
import pandas as pd
import numpy as np
from sklearn.metrics import log_loss
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier

# Read input data
data = pd.read_csv('input/train.csv')

# Split data into training, validation, and test sets
train_data, val_data, test_data = data.split(0.80, shuffle=True)
train_data, test_data = train_data.split(0.5, shuffle=True)

# Extract features and target
X = data[['EAP', 'HPL', 'MWS']]
y = data['author']

# Split into training and validation
X_train, y_train = X[:val_data.shape[0]], y[:val_data.shape[0]]
X_val, y_val = X[val_data.shape[0]:], y[val_data.shape[0]:]

# Initialize model
model = LGBMClassifier()

# Train model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_val)

# Evaluate
loss = log_loss(y_val, model.predict_proba(X_val))
print(f'Validation Loss: {loss}')

# Generate predictions for test data
y_test = model.predict(X_test)

# Save predictions
np.savetxt('submission/submission.csv', y_test, delimiter=',')
```