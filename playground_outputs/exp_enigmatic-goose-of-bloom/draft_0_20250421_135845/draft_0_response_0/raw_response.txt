Okay, so I need to solve this Kaggle competition problem where I have to predict the author of a sentence as one of three: Poe, Lovecraft, or Shelley. The task is to build a machine learning model that can accurately classify the author of a given text. 

First, I should understand the problem thoroughly. The goal is to predict the author based on the text content. The data provided includes three columns: EAP, HPL, and MWS, which are float values representing different metrics. These metrics likely correspond to the author's style or characteristics. The task is to use this data to predict the author.

I need to plan the approach. Since the task is classification with three possible outcomes, I can use a classification model. Multi-class classification is suitable here. I should consider different models and evaluate which one performs best.

Looking at the data, I see that the input has three columns with float values. These might be indicators of the author's style. I should preprocess the data to extract features from the text. Text features could include TF-IDF vectors, which capture the importance of words in the text. Alternatively, using word embeddings like Word2Vec or GloVe could also be effective. These features can then be used to train a classifier.

I should also consider the evaluation metric. The problem mentions using multi-class logarithmic loss, which is suitable for classification tasks with multiple classes. It measures the performance where the prediction doesn't need to predict the probability of the correct class, but rather the class itself. So, I'll use multi-class logistic regression or a similar model to evaluate the performance.

For the model, I can start with a simple logistic regression model, but since it's for multi-class, I might need to use a different approach. Alternatively, using a decision tree or random forest could be effective, but they are more suited for smaller datasets. Given the size of the dataset, I might need to use a more robust model like XGBoost or LightGBM, which are efficient and can handle large datasets well.

I should also think about feature engineering. Extracting TF-IDF features from the text could capture the importance of words, which might correlate with the author's style. Alternatively, using word embeddings could provide a numerical representation of the text that the model can learn from.

I need to ensure that the data is properly preprocessed. This includes handling missing values, normalizing the float values, and splitting the data into training and validation sets to evaluate the model's performance.

In terms of code structure, I'll need to load the data, extract features, train the model, and then evaluate it. The code should be concise, as per the user's request, and include a print statement for the evaluation metric.

I should also make sure that the predictions are saved to the correct path, as per the task requirements. The code should be well-structured and follow best practices for machine learning.

Now, considering the evaluation metric, multi-class logarithmic loss is appropriate. I'll calculate this metric after making predictions and print it clearly. This will give a quantitative measure of how well the model is performing.

In summary, the plan is to extract text features from the input data, train a multi-class classification model, evaluate it using multi-class logarithmic loss, and ensure the predictions are saved correctly.
</think>

```python
# Plan
1. Understand the task: Predict the author of a sentence as one of three: Poe, Lovecraft, or Shelley.
2. Use multi-class classification with appropriate evaluation metric.
3. Extract text features from the input text to train the model.
4. Implement a simple yet effective model using XGBoost or LightGBM for multi-class classification.
5. Save predictions to the required path after evaluation.

# Code
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import multi_class
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, log_loss

# Load data
data = pd.read_csv('input/sample_submission.csv')

# Extract features
text = data['text'].values
text_features = TfidfVectorizer(max_features=1000).fit_transform(text)

# Split data
X = text_features
y = data['author']

# Split into training and validation sets
X_train, X_text, y_train, y_text = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
gb = GradientBoostingClassifier(n_estimators=100, max_depth=10, random_state=42)
gb.fit(X_train, y_train)

# Make predictions
y_pred = gb.predict(X_text)

# Evaluate
loss = log_loss(y_text, multi_class(gb, X_text))
print(f"Logarithmic Loss: {loss:.4f}")
```