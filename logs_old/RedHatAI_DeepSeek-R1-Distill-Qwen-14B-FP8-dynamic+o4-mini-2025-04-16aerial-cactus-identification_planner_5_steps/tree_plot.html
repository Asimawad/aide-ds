<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [], "layout": [[0.0, 1.0]], "plan": ["## Task Summary: The task involves creating a machine learning model to classify\naerial images and determine if they contain a specific type of cactus\n(Neobuxbaumia tetetzo). The dataset consists of resized 32x32 images, with\ntraining labels provided in `train.csv` and test images in `test/`. The primary\nobjective is to predict the probability of each test image containing a cactus,\nevaluated using the area under the ROC curve (AUC). The solution must be\nimplemented using Kaggle Kernels and avoids Exploratory Data Analysis (EDA).\n---  ## Plan: 1. **Data Loading and Initial Setup**:      - Use\n`pandas.read_csv()` to load `train.csv` into a DataFrame `train_df` and\n`test.csv` into `test_df`.      - Extract image IDs from `train_df` and\n`test_df` for mapping to image files.      - Store test image IDs in a variable\n`test_ids` for later prediction submission.    2. **Dataset Creation**:      -\nDefine a custom `Dataset` class `CactusDataset` using `torch.utils.data.Dataset`\nto load images from the `train/` and `test/` directories.      - In the\n`__init__` method, store the image IDs, labels (for training), and root\ndirectory.      - In the `__getitem__` method, load images using\n`PIL.Image.open()`, convert to grayscale or RGB (as needed), and apply\nnormalization.    3. **Data Preprocessing**:      - Apply transformations such\nas resizing, normalization, and data augmentation (if needed) using\n`torchvision.transforms`.      - Normalize images to have zero mean and unit\nvariance.    4. **Model Architecture Definition**:      - Define a simple CNN\nmodel using `torch.nn.Sequential` with layers like `Conv2d`, `MaxPool2d`,\n`Dropout`, and `Linear` layers.      - Use `nn.Sigmoid` for the final output\nlayer to produce probability estimates.    5. **Loss Function and Optimizer\nSetup**:      - Use `nn.BCELoss()` as the loss function for binary\nclassification.      - Use the Adam optimizer with a learning rate (e.g.,\n0.001).    6. **Train-Validation Split**:      - Split the training data into\ntraining and validation sets using `torch.utils.data.random_split()`.      - Use\nthe same transformations for both sets to maintain consistency.    7. **Data\nLoaders Initialization**:      - Create ` DataLoader` instances for the training\nand validation datasets with batch size (e.g., 32) and shuffle the training\nloader.    8. **Model Training**:      - Train the model for a fixed number of\nepochs (e.g., 10).      - For each epoch, iterate over the training loader,\ncompute predictions, calculate loss, backpropagate, and update weights.      -\nEvaluate the model on the validation set after each epoch to track performance.\n9. **Model Evaluation on Test Set**:      - Use the trained model to make\npredictions on the test set.      - Save the predictions in the required format\n`id,has_cactus` with probabilities.    10. **Submission File Preparation**:\n- Combine the image IDs from the test set with their predicted probabilities\ninto a DataFrame.       - Save the DataFrame to a CSV file named\n`submission.csv` in the required format.    11. **Final Submission**:       -\nUpload the `submission.csv` file to Kaggle to evaluate the model's performance\nusing the AUC metric.    This plan provides a clear, step-by-step approach to\nbuilding a functional and accurate model for the competition, focusing on\nsimplicity and correctness for the initial submission."], "code": ["# Script generated by AIDE PlannerAgent (Chained Coder) - Step 1\n# Competition: aerial-cactus-identification\n# Task Summary:  ...\n# --- Master Plan ---\n# The task involves creating a machine learning model to classify aerial images and determine if they contain a specific type of cactus (Neobuxbaumia tetetzo). The dataset consists of resized 32x32 images, with training labels provided in `train.csv` and test images in `test/`. The primary objective is to predict the probability of each test image containing a cactus, evaluated using the area under the ROC curve (AUC). The solution must be implemented using Kaggle Kernels and avoids Exploratory Data Analysis (EDA).\n# ---\n# 1. **Data Loading and Initial Setup**:\n# - Use `pandas.read_csv()` to load `train.csv` into a DataFrame `train_df` and `test.csv` into `test_df`.\n# - Extract image IDs from `train_df` and `test_df` for mapping to image files.\n# - Store test image IDs in a variable `test_ids` for later prediction submission.\n# 2. **Dataset Creation**:\n# - Define a custom `Dataset` class `CactusDataset` using `torch.utils.data.Dataset` to load images from the `train/` and `test/` directories.\n# - In the `__init__` method, store the image IDs, labels (for training), and root directory.\n# - In the `__getitem__` method, load images using `PIL.Image.open()`, convert to grayscale or RGB (as needed), and apply normalization.\n# 3. **Data Preprocessing**:\n# - Apply transformations such as resizing, normalization, and data augmentation (if needed) using `torchvision.transforms`.\n# - Normalize images to have zero mean and unit variance.\n# 4. **Model Architecture Definition**:\n# - Define a simple CNN model using `torch.nn.Sequential` with layers like `Conv2d`, `MaxPool2d`, `Dropout`, and `Linear` layers.\n# - Use `nn.Sigmoid` for the final output layer to produce probability estimates.\n# 5. **Loss Function and Optimizer Setup**:\n# - Use `nn.BCELoss()` as the loss function for binary classification.\n# - Use the Adam optimizer with a learning rate (e.g., 0.001).\n# 6. **Train-Validation Split**:\n# - Split the training data into training and validation sets using `torch.utils.data.random_split()`.\n# - Use the same transformations for both sets to maintain consistency.\n# 7. **Data Loaders Initialization**:\n# - Create ` DataLoader` instances for the training and validation datasets with batch size (e.g., 32) and shuffle the training loader.\n# 8. **Model Training**:\n# - Train the model for a fixed number of epochs (e.g., 10).\n# - For each epoch, iterate over the training loader, compute predictions, calculate loss, backpropagate, and update weights.\n# - Evaluate the model on the validation set after each epoch to track performance.\n# 9. **Model Evaluation on Test Set**:\n# - Use the trained model to make predictions on the test set.\n# - Save the predictions in the required format `id,has_cactus` with probabilities.\n# 10. **Submission File Preparation**:\n# - Combine the image IDs from the test set with their predicted probabilities into a DataFrame.\n# - Save the DataFrame to a CSV file named `submission.csv` in the required format.\n# 11. **Final Submission**:\n# - Upload the `submission.csv` file to Kaggle to evaluate the model's performance using the AUC metric.\n# This plan provides a clear, step-by-step approach to building a functional and accurate model for the competition, focusing on simplicity and correctness for the initial submission.\n# --- End Master Plan ---\n\n# Thought: Importing standard libraries for filesystem operations, randomness, and data handling as outlined in Plan step 1.\nimport os\nimport random\n\n# Thought: Core data science libraries for numerical computations and dataframes (Plan step 1).\nimport numpy as np\nimport pandas as pd\n\n# Thought: Importing sklearn's train_test_split for later creating train/validation sets (Plan step 6).\nfrom sklearn.model_selection import train_test_split\n\n# Thought: PyTorch imports for model building, training, and data pipelines (Plan steps 2\u20139).\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as transforms\n\n# Thought: Image processing libraries; PIL for basic image I/O and cv2 for any OpenCV-based ops (Plan steps 2\u20133).\nfrom PIL import Image\nimport cv2\n\n# Thought: Pretrained model library for potential use of advanced architectures (Plan step 4).\nimport timm\n\n# Thought: Augmentation library to define complex transforms if needed (Plan step 3).\nimport albumentations as A\n\n\n# Thought: Define a reproducibility function to fix seeds across Python, NumPy, and PyTorch (CPU & GPU).\ndef set_seed(seed_value: int):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n    # Ensure deterministic behavior for CUDA (at potential cost of performance)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# Thought: Set a global seed for reproducibility as per best practice.\nset_seed(42)\n\n# Thought: Define DEVICE to leverage GPU if available, else fallback to CPU (Plan steps 4\u20139).\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Thought: Define base paths for input data and image directories (Master Plan Step 1).\nINPUT_DIR = os.path.join(\".\", \"input\")\nTRAIN_IMG_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_IMG_DIR = os.path.join(INPUT_DIR, \"test\")\n\n# Thought: Define full paths to primary CSV files: training labels, test IDs, and sample submission format (Master Plan Step 1).\nTRAIN_CSV_PATH = os.path.join(INPUT_DIR, \"train.csv\")\nTEST_CSV_PATH = os.path.join(INPUT_DIR, \"test.csv\")\nSAMPLE_SUBMISSION_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n\n# Thought: Load the main DataFrames for training labels, test IDs, and submission template (Master Plan Step 1).\ntrain_df = pd.read_csv(TRAIN_CSV_PATH)\ntest_df = pd.read_csv(TEST_CSV_PATH)\nsubmission_df = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n\n# Thought: Import ToTensorV2 for albumentations \u2192 PyTorch tensor conversion (Master Plan Step 3).\nfrom albumentations.pytorch import ToTensorV2\n\n# Thought: Define augmentation & normalization pipelines for train/val/test (Master Plan Step 3).\ntrain_transform = A.Compose(\n    [\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Rotate(limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\n\nval_transform = A.Compose(\n    [A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n)\n\ntest_transform = val_transform  # same as validation (no augmentation)\n\n\n# Thought: Custom Dataset for train/validation including labels (Master Plan Step 2).\nclass CactusDataset(Dataset):\n    def __init__(self, img_ids, labels, img_dir, transform=None):\n        self.img_ids = img_ids\n        self.labels = labels\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        # load image with OpenCV and convert BGR\u2192RGB\n        img_path = os.path.join(self.img_dir, img_id)\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # apply augmentation / preprocessing\n        if self.transform:\n            image = self.transform(image=image)[\"image\"]\n\n        # get label\n        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return image, label\n\n\n# Thought: Custom Dataset for test set (no labels, return id for submission) (Master Plan Step 2).\nclass CactusTestDataset(Dataset):\n    def __init__(self, img_ids, img_dir, transform=None):\n        self.img_ids = img_ids\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        img_path = os.path.join(self.img_dir, img_id)\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform:\n            image = self.transform(image=image)[\"image\"]\n\n        return image, img_id\n\n\n# Thought: Split original train_df into train/validation with stratification (Master Plan Step 6).\ntrain_split, val_split = train_test_split(\n    train_df, test_size=0.2, stratify=train_df[\"has_cactus\"], random_state=42\n)\ntrain_ids, train_labels = train_split[\"id\"].tolist(), train_split[\"has_cactus\"].tolist()\nval_ids, val_labels = val_split[\"id\"].tolist(), val_split[\"has_cactus\"].tolist()\ntest_ids = test_df[\"id\"].tolist()\n\n# Thought: Instantiate Datasets with appropriate transforms (Master Plan Steps 2\u20133).\ntrain_dataset = CactusDataset(\n    img_ids=train_ids,\n    labels=train_labels,\n    img_dir=TRAIN_IMG_DIR,\n    transform=train_transform,\n)\nval_dataset = CactusDataset(\n    img_ids=val_ids, labels=val_labels, img_dir=TRAIN_IMG_DIR, transform=val_transform\n)\ntest_dataset = CactusTestDataset(\n    img_ids=test_ids, img_dir=TEST_IMG_DIR, transform=test_transform\n)\n\n# Thought: Create DataLoader objects for train, validation, and test (Master Plan Step 7).\nbatch_size = 32\nnum_workers = 4\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,\n    pin_memory=True,\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True,\n)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True,\n)\n\n# At this point:\n# - train_loader, val_loader, test_loader are ready for model training & inference.\n\n# Thought: Define a simple CNN architecture using PyTorch per Master Plan Step 4.\nclass CactusCNN(nn.Module):\n    def __init__(self):\n        super(CactusCNN, self).__init__()\n        # feature extractor: three conv blocks with pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # 32x16x16\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 64x8x8\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 128x4x4\n        )\n        # classifier head: dropout, flatten, FC \u2192 Sigmoid for probability\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Flatten(),\n            nn.Linear(128 * 4 * 4, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(128, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        # squeeze the output to (batch,) shape for BCELoss compatibility\n        return x.view(-1)\n\n\n# Thought: Instantiate the model and move it to the configured DEVICE (CPU or GPU).\nmodel = CactusCNN().to(DEVICE)\n\n# Thought: Import AUC metric from sklearn for validation evaluation (Plan Step 8).\nfrom sklearn.metrics import roc_auc_score\n\n# Thought: Define training parameters as per Master Plan (Plan Steps 5 & 8).\nNUM_EPOCHS = 10\nLEARNING_RATE = 0.001\n\n# Thought: Setup loss function and optimizer (Plan Step 5).\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Thought: Variable to track the best validation AUC for model checkpointing.\nbest_val_auc = 0.0\n\n# Thought: Main training loop over epochs (Plan Step 8).\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss = 0.0\n\n    # Thought: Iterate over training batches, compute loss, and update weights.\n    for images, labels in train_loader:\n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        optimizer.zero_grad()\n        outputs = model(images)  # forward pass\n        loss = criterion(outputs, labels)  # compute BCELoss\n        loss.backward()  # backpropagation\n        optimizer.step()  # weight update\n\n        running_loss += loss.item() * images.size(0)\n\n    epoch_train_loss = running_loss / len(train_loader.dataset)\n\n    # Thought: Validation phase: evaluate loss and AUC without gradient tracking.\n    model.eval()\n    val_running_loss = 0.0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(DEVICE)\n            labels = labels.to(DEVICE)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_running_loss += loss.item() * images.size(0)\n\n            # store predictions and true labels for AUC\n            all_preds.extend(outputs.cpu().numpy().tolist())\n            all_labels.extend(labels.cpu().numpy().tolist())\n\n    epoch_val_loss = val_running_loss / len(val_loader.dataset)\n    epoch_val_auc = roc_auc_score(all_labels, all_preds)\n\n    # Thought: Checkpoint the model if validation AUC improves (optional per Plan).\n    if epoch_val_auc > best_val_auc:\n        best_val_auc = epoch_val_auc\n        torch.save(model.state_dict(), \"best_model.pth\")\n\n    # Thought: Print epoch metrics for monitoring progress.\n    print(\n        f\"Epoch {epoch+1}/{NUM_EPOCHS} - \"\n        f\"Train Loss: {epoch_train_loss:.4f} - \"\n        f\"Val Loss: {epoch_val_loss:.4f} - \"\n        f\"Val Metric (AUC): {epoch_val_auc:.4f}\"\n    )\n\n# Thought: Load the best model weights saved during training (Master Plan Step 9 checkpoint)\nmodel.load_state_dict(torch.load(\"best_model.pth\", map_location=DEVICE))\n\n# Thought: Move model to DEVICE and set to evaluation mode for inference\nmodel.to(DEVICE)\nmodel.eval()\n\n# Thought: Prepare containers for predictions and corresponding test IDs\nall_preds = []\nall_ids = []\n\n# Thought: Disable gradient computation for faster inference\nwith torch.no_grad():\n    for images, img_ids in test_loader:\n        # Thought: Move input batch to the configured DEVICE\n        images = images.to(DEVICE)\n\n        # Thought: Forward pass to get probability outputs (model already ends with Sigmoid)\n        outputs = model(images)\n\n        # Thought: Transfer predictions back to CPU and convert to list\n        batch_preds = outputs.cpu().numpy().tolist()\n\n        # Thought: Accumulate batch predictions and their image IDs\n        all_preds.extend(batch_preds)\n        all_ids.extend(img_ids)\n\n# Thought: Create a DataFrame in the required submission format (Master Plan Step 10)\nsubmission = pd.DataFrame({\"id\": all_ids, \"has_cactus\": all_preds})\n\n# Thought: Ensure the submission directory exists\nos.makedirs(\"./submission\", exist_ok=True)\n\n# Thought: Save the submission DataFrame to CSV without the index column\nsubmission.to_csv(\"./submission/submission.csv\", index=False)"], "term_out": ["<OMITTED>"], "analysis": ["The script failed with a FileNotFoundError when attempting to read `test.csv`, so no submission file was produced."], "exp_name": "RedHatAI_DeepSeek-R1-Distill-Qwen-14B-FP8-dynamic+o4-mini-2025-04-16aerial-cactus-identification_planner_5_steps", "metrics": [0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
